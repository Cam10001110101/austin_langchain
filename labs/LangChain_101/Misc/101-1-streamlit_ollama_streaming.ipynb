{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ns1-iCD0PMta"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/colinmcnamara/austin_langchain/blob/main/labs/LangChain_101/Misc/101-1-streamlit_ollama_streaming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TLMrWVStFdAx",
    "outputId": "072ea48f-09b7-4048-8ffe-55432b61fe51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "%pip install -q langchain streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lAm3euIZFLSB",
    "outputId": "7168405b-2d0b-414a-969c-d08aa2d2184b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting streaming_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile streaming_app.py\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "import urllib.request, json\n",
    "import streamlit as st\n",
    "\n",
    "@st.cache_resource\n",
    "def get_ollama_models(ollama_server_url):\n",
    "    api_tags = \"/api/tags\"\n",
    "    models = []\n",
    "    with urllib.request.urlopen(ollama_server_url + api_tags) as tags:\n",
    "        response = json.load(tags)\n",
    "        models = [model['name'] for model in response['models']]\n",
    "\n",
    "        # This is not necessary but added here to keep the output clean\n",
    "        models = [model.replace(\":latest\",\"\") for model in models]\n",
    "    return tuple(models)\n",
    "\n",
    "class StreamHandler(BaseCallbackHandler):\n",
    "    def __init__(self, container, initial_text=\"\"):\n",
    "        self.container = container\n",
    "        self.text = initial_text\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        self.text += token\n",
    "        self.container.markdown(self.text)\n",
    "\n",
    "st.title(\"LangChain + Streamlit + Ollama\")\n",
    "\n",
    "with st.sidebar:\n",
    "    # Url of the hosted ollama instance.\n",
    "    ollama_server = st.text_input(\"Ollama API Server\", value=\"http://localhost:11434\")\n",
    "    ollama_model = st.selectbox(\"Ollama model name\", get_ollama_models(ollama_server), index=None)\n",
    "\n",
    "    st.markdown(\"\"\"\n",
    "    Changing the model doesn't clear the history which gets passed to the new llm as context.\n",
    "    Use the below button to clear chat history and start with a clear slate.\n",
    "    \"\"\")\n",
    "    if st.button(\"Clear chat history\", type=\"primary\"):\n",
    "        st.session_state[\"messages\"] = [AIMessage(content=\"How can I help you?\")]\n",
    "\n",
    "\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state[\"messages\"] = [AIMessage(content=\"How can I help you?\")]\n",
    "\n",
    "for msg in st.session_state.messages:\n",
    "    st.chat_message(msg.type).write(msg.content)\n",
    "\n",
    "if prompt := st.chat_input():\n",
    "    st.session_state.messages.append(HumanMessage(content=prompt))\n",
    "    st.chat_message(\"user\").write(prompt)\n",
    "\n",
    "    if not ollama_model:\n",
    "        st.info(\"\"\"Please select an existing Ollama model name to continue.\n",
    "        Visit https://ollama.ai/library for a list of supported models.\n",
    "        Restart the streamlit app after downloading a model using the `ollama pull <model_name>` command.\n",
    "        It should become available in the list of available models.\"\"\")\n",
    "        st.stop()\n",
    "\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        stream_handler = StreamHandler(st.empty())\n",
    "        llm = ChatOllama(model=ollama_model, streaming=True, callbacks=[stream_handler])\n",
    "        response = llm(st.session_state.messages)\n",
    "        st.session_state.messages.append(AIMessage(content=response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFa0Fpq-PMtf"
   },
   "source": [
    "### Download and run ollama\n",
    "\n",
    "Below, we:\n",
    "1. download the ollama binary\n",
    "2. make it executable\n",
    "3. start ollama in the background\n",
    "4. download the hosted mistral model\n",
    "5. download the hosted llama2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dA6_BjFEPMtf"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!curl -L https://ollama.ai/download/ollama-linux-amd64 -o ollama\n",
    "!chmod +x ollama\n",
    "!./ollama serve &>/content/ollama_logs.txt &\n",
    "!./ollama pull mistral\n",
    "!./ollama pull llama2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start and background streamlit app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YeNLTTJSY5NM"
   },
   "outputs": [],
   "source": [
    "!streamlit run streaming_app.py &>/content/logs.txt &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifux-mmzcTQK"
   },
   "source": [
    "## Find the IP of your instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nQz0WaTTcTQK",
    "outputId": "99462135-a0aa-41c9-f0cf-a21e0eb144e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.227.3.82\n",
      "Copy this IP into the webpage that opens below\n"
     ]
    }
   ],
   "source": [
    "!curl ipv4.icanhazip.com\n",
    "!echo \"Copy this IP into the webpage that opens below\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1XkNhk9abV29"
   },
   "source": [
    "## Expose the Streamlit app on port 8501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_DDegT-jZ2Q5",
    "outputId": "e319b466-95b7-4e6d-e07e-a6eb3a6eb1a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[?25hnpx: installed 22 in 3.73s\n",
      "your url is: https://common-mails-doubt.loca.lt\n"
     ]
    }
   ],
   "source": [
    "!npx localtunnel --port 8501"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
