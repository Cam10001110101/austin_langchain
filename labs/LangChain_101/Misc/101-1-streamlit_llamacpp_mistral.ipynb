{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF3dgiwHhJ9x"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/colinmcnamara/austin_langchain/blob/main/labs/LangChain_101/Misc/101-1-streamlit_llamacpp_mistral.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Install langchain, streamlit, and huggingface_hub modules for python"
      ],
      "metadata": {
        "id": "OwzYHw1ckDaK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TLMrWVStFdAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d879f912-1f53-468a-ad66-1d9b8e8d57c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m115.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -q langchain huggingface_hub streamlit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We will use [Mistral 7B Instruct v0.1 model](https://mistral.ai/) but since those models can be very large, we'll instead use a quantized (compressed) version from [Tom Jobins](https://huggingface.co/TheBloke) on [Hugging Face](https://huggingface.co/)\n",
        "* The compressed version is still 4GB in size. So we'll pre-fetch it and cache it now so we can see the download progress. That way, when we call this again from Streamlit, we'll already have it on disk and it won't take nearly as long to start the streamlit app, as it does to download the model."
      ],
      "metadata": {
        "id": "SHwdWfsvkNir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "(repo_id, model_file_name) = (\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\", \"mistral-7b-instruct-v0.1.Q4_0.gguf\")\n",
        "hf_hub_download(repo_id=repo_id, filename=model_file_name, repo_type=\"model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "2c3655794c194ca78fd33752a3a9bdf7",
            "a3d14fe3b88c4c539aadb9970e7f8198",
            "7e6b8219c19147a084499844f592b67b",
            "8e0014a613bb49909eef11642b7d9871",
            "d98637feca1e4ff9b24c8db50dac3e54",
            "909ac2348d2e4dd6afda4d888b50197e",
            "59aad88395304280b88a68c20f9f81ec",
            "e593168368314151ad81a4322f1eaf0f",
            "73bfa54f903a435490cb12a23018cfd8",
            "3607082e51004252a6fd877b00b0bbcb",
            "fd36fb49396b4d84b2278f744712be51"
          ]
        },
        "id": "GpbvaHY1zcPB",
        "outputId": "e8e96ebd-cafb-4d7a-8303-9c8ec03e147e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "mistral-7b-instruct-v0.1.Q4_0.gguf:   0%|          | 0.00/4.11G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c3655794c194ca78fd33752a3a9bdf7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/root/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.1-GGUF/snapshots/45167a542b6fa64a14aea61a4c468bbbf9f258a8/mistral-7b-instruct-v0.1.Q4_0.gguf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We will install [LlamaCpp](https://python.langchain.com/docs/integrations/llms/llamacpp) for python but with CUDA support so we can leverage GPU.\n",
        "* This model will also work without GPU support on standard CPU, but then it will be very slow to process our requests and respond back.\n",
        "* This notebook has code that requires GPU so ensure you have the `T4 GPU` selected. Otherwise the program will crash without any errors on screen."
      ],
      "metadata": {
        "id": "kfYBa9knk_YS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILVCePOuhmXH",
        "outputId": "c349d7ab-6f32-4f0e-c67c-9487868ebeda"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.15.tar.gz (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.23.5)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.15-cp310-cp310-manylinux_2_35_x86_64.whl size=6969351 sha256=2eee8720722d9fbb2cf7571c75a1015f7108eb4954eebf244375e3995bb0d7af\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/9a/85/b27890418a82fb5be7ceddff8e60f573f6ce989f7a2b43f7ca\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: llama-cpp-python\n",
            "Successfully installed llama-cpp-python-0.2.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The original code modified and tweaked to use LlamaCpp so we can use local llm instead of OpenAI."
      ],
      "metadata": {
        "id": "NUiz3YallRNg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAm3euIZFLSB",
        "outputId": "785ef14f-975f-4980-a41c-d2bf629ec828"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing streaming_app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile streaming_app.py\n",
        "import streamlit as st\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.callbacks.base import BaseCallbackHandler\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "\n",
        "# We don't want to reinitialize the llm with every user interaction so we ask Streamlit to cache it\n",
        "# Downloading a model takes time so we only want to do it once and cache it.\n",
        "# Once a model is downloaded, it has to be loaded, and the time it takes to load a model is directly\n",
        "# proportional to its physical size. So we don't want to keep reloading the model. Hence, we ask streamlit\n",
        "# to cache it instead.\n",
        "@st.cache_resource\n",
        "def download_model_and_prepare_llm():\n",
        "\n",
        "    # We have already run these 2 lines earlier, so when Streamlit runs it again, since it already has the model\n",
        "    # downloaded locally, it will do an early return with the model's path on disk\n",
        "\n",
        "    (repo_id, model_file_name) = (\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\", \"mistral-7b-instruct-v0.1.Q4_0.gguf\")\n",
        "\n",
        "    model_path = hf_hub_download(repo_id=repo_id, filename=model_file_name, repo_type=\"model\")\n",
        "\n",
        "    # We will now load the model using the LlamaCpp interface which we will later pass to LangChain\n",
        "    llama_llm = LlamaCpp(\n",
        "            model_path=model_path,\n",
        "            temperature=0,\n",
        "            max_tokens=512,\n",
        "            top_p=1,\n",
        "            # callback_manager=callback_manager,\n",
        "            n_gpu_layers=100,\n",
        "            n_batch=512,\n",
        "            n_ctx=4096,\n",
        "            stop=[\"[INST]\"],\n",
        "            verbose=False,\n",
        "            streaming=True,\n",
        "            )\n",
        "\n",
        "    return llama_llm\n",
        "\n",
        "\n",
        "# We cache the llm chain until the system prompt changes. Then we re initialize it with the new prompt.\n",
        "@st.cache_resource\n",
        "def create_chain(system_prompt):\n",
        "\n",
        "    # We get the model we pre-fetched earlier and loaded using LlamaCpp interface\n",
        "    llama_llm = download_model_and_prepare_llm()\n",
        "\n",
        "    # Mistral 7b is a foundational model. It won't work with chat prompt out of the box.\n",
        "    # So we create a new prompt template based on their specifications.\n",
        "    # We also pass in an initial \"system prompt\" to set the model's personality.\n",
        "    template = \"\"\"<s>[INST]{}[/INST]</s>\n",
        "\n",
        "[INST]{}[/INST]\"\"\".format(system_prompt, \"{question}\")\n",
        "\n",
        "    # We now prepare the prompt template using the template string above so we can create an llm chain using it\n",
        "    prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "    # We initialize the chain with the model and the prompt template.\n",
        "    # llm_chain = LLMChain(llm=llama_llm, prompt=prompt)\n",
        "    llm_chain = prompt | llama_llm\n",
        "\n",
        "    return llm_chain\n",
        "\n",
        "\n",
        "# We don't use stream handler at this time.\n",
        "class StreamHandler(BaseCallbackHandler):\n",
        "    def __init__(self, container, initial_text=\"\"):\n",
        "        self.container = container\n",
        "        self.text = initial_text\n",
        "\n",
        "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
        "        self.text += token\n",
        "        self.container.markdown(self.text)\n",
        "\n",
        "\n",
        "# Create a sidebar text field to allow us to set and modify the initial prompt\n",
        "with st.sidebar:\n",
        "    system_prompt = st.text_area(label=\"Enter a system prompt to adjust the chatbot behavior\",\n",
        "                                 value=\"You are a helpful AI assistant who answers questions in short sentences.\")\n",
        "\n",
        "# We build our chain by passing the system prompt.\n",
        "# It is worth noting that the chain will be rebuilt if we change the system prompt.\n",
        "llm = create_chain(system_prompt)\n",
        "\n",
        "# We initialize the chat history in streamlit session\n",
        "# This is not the model's memory. Just a list of messages we passed to it and\n",
        "# responses we received from it. This will be used to render the chat history.\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state[\"messages\"] = [{\"role\": \"assistant\", \"content\": \"How may I help you today?\"}]\n",
        "\n",
        "# We loop though the chat history in the session and display it in a chat like interface\n",
        "for msg in st.session_state.messages:\n",
        "    st.chat_message(msg[\"role\"]).write(msg[\"content\"])\n",
        "\n",
        "# When we receive user input from the chat input:\n",
        "if prompt := st.chat_input():\n",
        "\n",
        "    # 1. we add it to the chat history in the session memory and tag it as user message\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    # 2. we add it to the chat history on the screen\n",
        "    st.chat_message(\"user\").write(prompt)\n",
        "\n",
        "    # stream_handler = StreamHandler(st.empty())\n",
        "\n",
        "    # 3. pass it to our llm chain as a prompt and get a response back\n",
        "    # response = llm.run(prompt)\n",
        "    response = llm.invoke({\"question\":prompt})\n",
        "\n",
        "    # 4. we add the response to the chat history in the session memory and tag it as AI response\n",
        "    st.session_state.messages.append({\"role\":\"assistant\", \"content\": response})\n",
        "\n",
        "    # 5. We append the response to the chat history on the screen\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.markdown(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YeNLTTJSY5NM"
      },
      "outputs": [],
      "source": [
        "!streamlit run streaming_app.py >>/content/logs.txt 2>&1 &"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifux-mmzcTQK"
      },
      "source": [
        "## Find the IP of your instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQz0WaTTcTQK",
        "outputId": "a27892e9-814d-4a2b-9399-80e2ebc58f13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.125.185.186\n",
            "Copy this IP into the webpage that opens below\n"
          ]
        }
      ],
      "source": [
        "!curl ipv4.icanhazip.com\n",
        "!echo \"Copy this IP into the webpage that opens below\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XkNhk9abV29"
      },
      "source": [
        "## Expose the Streamlit app on port 8501"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_DDegT-jZ2Q5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cac368e5-4981-453b-8523-4cdb74b6c6b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 4.225s\n",
            "your url is: https://quiet-llamas-battle.loca.lt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2c3655794c194ca78fd33752a3a9bdf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a3d14fe3b88c4c539aadb9970e7f8198",
              "IPY_MODEL_7e6b8219c19147a084499844f592b67b",
              "IPY_MODEL_8e0014a613bb49909eef11642b7d9871"
            ],
            "layout": "IPY_MODEL_d98637feca1e4ff9b24c8db50dac3e54"
          }
        },
        "a3d14fe3b88c4c539aadb9970e7f8198": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_909ac2348d2e4dd6afda4d888b50197e",
            "placeholder": "​",
            "style": "IPY_MODEL_59aad88395304280b88a68c20f9f81ec",
            "value": "mistral-7b-instruct-v0.1.Q4_0.gguf: 100%"
          }
        },
        "7e6b8219c19147a084499844f592b67b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e593168368314151ad81a4322f1eaf0f",
            "max": 4108916384,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_73bfa54f903a435490cb12a23018cfd8",
            "value": 4108916384
          }
        },
        "8e0014a613bb49909eef11642b7d9871": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3607082e51004252a6fd877b00b0bbcb",
            "placeholder": "​",
            "style": "IPY_MODEL_fd36fb49396b4d84b2278f744712be51",
            "value": " 4.11G/4.11G [02:43&lt;00:00, 46.6MB/s]"
          }
        },
        "d98637feca1e4ff9b24c8db50dac3e54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "909ac2348d2e4dd6afda4d888b50197e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59aad88395304280b88a68c20f9f81ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e593168368314151ad81a4322f1eaf0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73bfa54f903a435490cb12a23018cfd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3607082e51004252a6fd877b00b0bbcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd36fb49396b4d84b2278f744712be51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}