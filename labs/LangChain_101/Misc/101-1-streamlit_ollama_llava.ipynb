{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ns1-iCD0PMta"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/colinmcnamara/austin_langchain/blob/main/labs/LangChain_101/Misc/101-1-streamlit_ollama_llava.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TLMrWVStFdAx",
    "outputId": "072ea48f-09b7-4048-8ffe-55432b61fe51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "%pip install -q langchain streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lAm3euIZFLSB",
    "outputId": "7168405b-2d0b-414a-969c-d08aa2d2184b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting streaming_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile streaming_app.py\n",
    "import streamlit as st\n",
    "from langchain.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from operator import itemgetter\n",
    "import base64\n",
    "from langchain.schema import AIMessage, HumanMessage\n",
    "\n",
    "st.title(\"Multimodal Chat\")\n",
    "\n",
    "llm = Ollama(model=\"bakllava\")\n",
    "\n",
    "\n",
    "def bind_and_run_llm(payload):\n",
    "    image = payload[\"image\"]\n",
    "    prompt = payload[\"prompt\"]\n",
    "    bound = llm.bind(images=[image])\n",
    "    return bound.invoke(prompt)\n",
    "\n",
    "\n",
    "image_template = \"{image}\"\n",
    "image_prompt = PromptTemplate.from_template(image_template)\n",
    "prompt_template = \"{question}\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "chain = (\n",
    "    {\"image\": itemgetter(\"image\"), \"prompt\": prompt} |\n",
    "    RunnableLambda(bind_and_run_llm)\n",
    ")\n",
    "\n",
    "\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state[\"messages\"] = [\n",
    "            (AIMessage(content=\"How can I help you?\"), False)\n",
    "            ]\n",
    "\n",
    "if \"uploaded_file\" not in st.session_state:\n",
    "    st.session_state[\"uploaded_file\"] = None\n",
    "\n",
    "for msg in st.session_state.messages:\n",
    "    if not msg[1]:\n",
    "        st.chat_message(msg[0].type).write(msg[0].content)\n",
    "    else:\n",
    "        st.chat_message(msg[0].type).image(msg[0].data, width=200)\n",
    "\n",
    "if uploaded_file := st.sidebar.file_uploader(\"Upload an image file\",\n",
    "                                             type=[\"jpg\", \"png\"]):\n",
    "    if st.session_state.uploaded_file != uploaded_file:\n",
    "        st.session_state.uploaded_file = uploaded_file\n",
    "        st.session_state.messages.append(\n",
    "                (HumanMessage(\n",
    "                    content=uploaded_file.name,\n",
    "                    data=uploaded_file\n",
    "                    ), True))\n",
    "        st.chat_message(\"user\").image(uploaded_file, width=200)\n",
    "\n",
    "if prompt := st.chat_input():\n",
    "    st.session_state.messages.append((HumanMessage(content=prompt), False))\n",
    "    st.chat_message(\"human\").write(prompt)\n",
    "\n",
    "    response = \"\"\n",
    "    if uploaded_file is not None:\n",
    "        data = uploaded_file.getvalue()\n",
    "        b64 = base64.b64encode(data).decode()\n",
    "\n",
    "        response = chain.invoke({\"question\": prompt, \"image\": b64})\n",
    "    else:\n",
    "        response = \"Please upload an image first\"\n",
    "\n",
    "    st.session_state.messages.append((AIMessage(content=response), False))\n",
    "    st.chat_message(\"assistant\").write(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFa0Fpq-PMtf"
   },
   "source": [
    "### Download and run ollama\n",
    "\n",
    "Below, we:\n",
    "1. download the ollama binary\n",
    "2. make it executable\n",
    "3. start ollama in the background\n",
    "4. download the hosted bakllava model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dA6_BjFEPMtf"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!curl -L https://ollama.ai/download/ollama-linux-amd64 -o ollama\n",
    "!chmod +x ollama\n",
    "!./ollama serve &>/content/ollama_logs.txt &\n",
    "!./ollama pull bakllava"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start and background streamlit app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YeNLTTJSY5NM"
   },
   "outputs": [],
   "source": [
    "!streamlit run streaming_app.py &>/content/logs.txt &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifux-mmzcTQK"
   },
   "source": [
    "## Find the IP of your instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nQz0WaTTcTQK",
    "outputId": "99462135-a0aa-41c9-f0cf-a21e0eb144e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.227.3.82\n",
      "Copy this IP into the webpage that opens below\n"
     ]
    }
   ],
   "source": [
    "!curl ipv4.icanhazip.com\n",
    "!echo \"Copy this IP into the webpage that opens below\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1XkNhk9abV29"
   },
   "source": [
    "## Expose the Streamlit app on port 8501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_DDegT-jZ2Q5",
    "outputId": "e319b466-95b7-4e6d-e07e-a6eb3a6eb1a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[?25hnpx: installed 22 in 3.73s\n",
      "your url is: https://common-mails-doubt.loca.lt\n"
     ]
    }
   ],
   "source": [
    "!npx localtunnel --port 8501"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
