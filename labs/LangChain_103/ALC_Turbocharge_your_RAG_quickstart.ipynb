{"cells":[{"cell_type":"raw","id":"34814bdb-d05b-4dd3-adf1-ca5779882d7e","metadata":{"id":"34814bdb-d05b-4dd3-adf1-ca5779882d7e"},"source":[]},{"cell_type":"markdown","id":"bd231c0a","metadata":{},"source":["The source for this lab was from the LangChain teams updated documentation found here - https://python.langchain.com/docs/use_cases/question_answering/quickstart\n","\n","This has been modified to support using google colab secret keys, and a section added showing connection into google drive.\n","\n","Big thanks to the LangChain team for the doc updates. 0.1 ROCKS!!!!"]},{"cell_type":"markdown","id":"86fc5bb2-017f-434e-8cd6-53ab214a5604","metadata":{"id":"86fc5bb2-017f-434e-8cd6-53ab214a5604"},"source":["# Quickstart"]},{"cell_type":"markdown","id":"de913d6d-c57f-4927-82fe-18902a636861","metadata":{"id":"de913d6d-c57f-4927-82fe-18902a636861"},"source":["[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/colinmcnamara/austin_langchain/blob/main/labs/LangChain_103/ALC_Turbocharge_your_RAG_quickstart.ipynb)"]},{"cell_type":"markdown","id":"487d8d79-5ee9-4aa4-9fdf-cd5f4303e099","metadata":{"id":"487d8d79-5ee9-4aa4-9fdf-cd5f4303e099"},"source":["## Setup\n","\n","### Dependencies\n","\n","We'll use an OpenAI chat model and embeddings and a Chroma vector store in this walkthrough, but everything shown here works with any [ChatModel](/docs/modules/model_io/chat/) or [LLM](/docs/modules/model_io/llms/), [Embeddings](/docs/modules/data_connection/text_embedding/), and [VectorStore](/docs/modules/data_connection/vectorstores/) or [Retriever](/docs/modules/data_connection/retrievers/).\n","\n","We'll use the following packages:"]},{"cell_type":"code","execution_count":null,"id":"QG6Db_6nURWw","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13619,"status":"ok","timestamp":1705526948653,"user":{"displayName":"","userId":""},"user_tz":360},"id":"QG6Db_6nURWw","outputId":"628d40d6-66a7-403e-fe64-8c52547e0086"},"outputs":[{"name":"stdout","output_type":"stream","text":["  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for colab_env (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["%pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai chromadb bs4 colab_env"]},{"cell_type":"markdown","id":"51ef48de-70b6-4f43-8e0b-ab9b84c9c02a","metadata":{"id":"51ef48de-70b6-4f43-8e0b-ab9b84c9c02a"},"source":["We need to get our keys from Google Colab\n","If you are using a local enviornment, you can uncomment the the dotenv statements"]},{"cell_type":"code","execution_count":null,"id":"143787ca-d8e6-4dc9-8281-4374f4d71720","metadata":{"id":"143787ca-d8e6-4dc9-8281-4374f4d71720"},"outputs":[],"source":["import getpass\n","import os\n","from google.colab import userdata\n","os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n","\n","# Uncomment below if you want to enter the API key manually\n","#os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n","\n","# Uncomment below if you want to use .env file\n","# import dotenv\n","# dotenv.load_dotenv()"]},{"cell_type":"markdown","id":"1665e740-ce01-4f09-b9ed-516db0bd326f","metadata":{"id":"1665e740-ce01-4f09-b9ed-516db0bd326f"},"source":["### LangSmith\n","\n","Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith](https://smith.langchain.com).\n","\n","Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:"]},{"cell_type":"code","execution_count":null,"id":"07411adb-3722-4f65-ab7f-8f6f57663d11","metadata":{"id":"07411adb-3722-4f65-ab7f-8f6f57663d11"},"outputs":[],"source":["os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n","os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')\n"]},{"cell_type":"markdown","id":"fa6ba684-26cf-4860-904e-a4d51380c134","metadata":{"id":"fa6ba684-26cf-4860-904e-a4d51380c134"},"source":["## overview\n","\n","In this guide we'll build a QA app that uses a RAG chain to answer questions. We'll pull the transcript from our LangChain 101 virtual edition and use it to answer questions.\n","\n","We can create a simple indexing pipeline and RAG chain to do this in ~20 lines of code:"]},{"cell_type":"code","execution_count":null,"id":"d8a913b1-0eea-442a-8a64-ec73333f104b","metadata":{"id":"d8a913b1-0eea-442a-8a64-ec73333f104b"},"outputs":[],"source":["import bs4\n","from langchain import hub\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_community.document_loaders import WebBaseLoader\n","from langchain_community.vectorstores import Chroma\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n","from langchain.document_loaders import TextLoader\n"]},{"cell_type":"code","execution_count":null,"id":"MGlz_p5cUvCl","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17928,"status":"ok","timestamp":1705526986528,"user":{"displayName":"","userId":""},"user_tz":360},"id":"MGlz_p5cUvCl","outputId":"df3172e5-e414-40e5-d6de-8d4455071f8e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n"]},{"cell_type":"code","execution_count":null,"id":"b27c4404","metadata":{},"outputs":[],"source":["import os\n","import requests\n","\n","# Path to your Google Drive directory\n","directory_path = '/content/gdrive/My Drive/austin_langchain_labs'\n","\n","# Check if the directory exists within Google Drive\n","if not os.path.exists(directory_path):\n","    # If the directory does not exist, create it using the os.mkdir() as os.makedirs() might not work as expected\n","    os.mkdir(directory_path)\n","    print(f\"Directory {directory_path} created\")\n","else:\n","    print(f\"Directory {directory_path} already exists\")\n","\n","# URL of the file to be downloaded\n","file_url = \"https://github.com/colinmcnamara/austin_langchain/blob/main/resources/transcripts/langchain_101-Transcript.txt?raw=true\"\n","\n","# Filename to save the file as\n","filename = \"langchain_101-Transcript.txt\"\n","\n","# Full path to save the file\n","file_path = os.path.join(directory_path, filename)\n","\n","# Download the file\n","response = requests.get(file_url)\n","\n","# Check if the request was successful\n","if response.status_code == 200:\n","    with open(file_path, 'wb') as file:\n","        file.write(response.content)\n","    print(f\"File successfully saved to {file_path}\")\n","else:\n","    print(\"Failed to download the file.\")\n"]},{"cell_type":"code","execution_count":null,"id":"820244ae-74b4-4593-b392-822979dd91b8","metadata":{"id":"820244ae-74b4-4593-b392-822979dd91b8"},"outputs":[],"source":["# We load the text document from our Google Drive\n","loader = TextLoader(\"/content/gdrive/My Drive/austin_langchain_labs/langchain_101-Transcript.txt\")\n","\n","docs = loader.load()\n","\n","# We split the document into smaller chunks\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n","splits = text_splitter.split_documents(docs)\n","\n","# We embed the text snippets using OpenAi's embedding model and stuff it into a vectorstore\n","vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n","\n","# Retrieve and generate using the relevant snippets of the blog.\n","retriever = vectorstore.as_retriever()\n","\n","# We pull our rag prompt from the hub\n","prompt = hub.pull(\"rlm/rag-prompt\")\n","\n","# We specify gpt-3.5-turbo as our model and set the temperature to 0 to get the most coherent response\n","llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","\n","# we define a function to format the documents\n","def format_docs(docs):\n","    return \"\\n\\n\".join(doc.page_content for doc in docs)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"066f780e","metadata":{},"outputs":[],"source":["# Here we define our langchain pipeline using LCEL - note how simple it is like the Unix command line \n","rag_chain = (\n","    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n","    | prompt\n","    | llm\n","    | StrOutputParser()\n",")"]},{"cell_type":"code","execution_count":null,"id":"0d3b0f36-7b56-49c0-8e40-a1aa9ebcbf24","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"elapsed":1736,"status":"ok","timestamp":1705527045030,"user":{"displayName":"","userId":""},"user_tz":360},"id":"0d3b0f36-7b56-49c0-8e40-a1aa9ebcbf24","outputId":"bcfebcdc-8a20-4285-dcf7-600ef1bd6d80"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done through various methods such as using prompting techniques, task-specific instructions, or human inputs. The goal is to make the task more manageable and facilitate the interpretation of the model's thinking process.\""]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["rag_chain.invoke(\"What did Ricky Say about caching in streamlit?\")"]},{"cell_type":"code","execution_count":null,"id":"7cb344e0-c423-400c-a079-964c08e07e32","metadata":{"id":"7cb344e0-c423-400c-a079-964c08e07e32"},"outputs":[],"source":["# cleanup\n","vectorstore.delete_collection()"]},{"cell_type":"markdown","id":"639dc31a-7f16-40f6-ba2a-20e7c2ecfe60","metadata":{"id":"639dc31a-7f16-40f6-ba2a-20e7c2ecfe60"},"source":[":::tip\n","\n","Check out the [LangSmith trace](https://smith.langchain.com/public/1c6ca97e-445b-4d00-84b4-c7befcbc59fe/r)\n","\n",":::"]}],"metadata":{"colab":{"provenance":[{"file_id":"1LXgVVMOqO-AC5jAYQD1-e2gwxeSqqEfF","timestamp":1705527521136},{"file_id":"https://github.com/langchain-ai/langchain/blob/master/docs/docs/use_cases/question_answering/quickstart.ipynb","timestamp":1705527330264}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"}},"nbformat":4,"nbformat_minor":5}
