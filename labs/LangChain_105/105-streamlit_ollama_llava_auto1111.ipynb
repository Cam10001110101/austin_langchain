{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ns1-iCD0PMta"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/colinmcnamara/austin_langchain/blob/main/labs/LangChain_105/105-streamlit_ollama_llava_auto1111.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TLMrWVStFdAx",
    "outputId": "072ea48f-09b7-4048-8ffe-55432b61fe51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "%pip install -q langchain streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting graph.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile graph.py\n",
    "import base64\n",
    "import json\n",
    "import operator\n",
    "import os\n",
    "import requests\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.messages import BaseMessage, AIMessage, HumanMessage, FunctionMessage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolInvocation, ToolExecutor\n",
    "from numpy import random\n",
    "from typing import Optional, TypedDict, Annotated, Sequence, Dict\n",
    "\n",
    "# default base urls for automatic 1111 and ollama\n",
    "a_1111_base_url = \"http://localhost:7860\"\n",
    "ollama_base_url = \"http://localhost:11434\"\n",
    "\n",
    "# environment variable names\n",
    "a_1111_env_key = \"AUTOMATIC1111_HOST_URL\"\n",
    "ollama_env_key = \"OLLAMA_HOST_URL\"\n",
    "\n",
    "# override values for automatic 1111 and ollama from\n",
    "# environment variables if present\n",
    "if a_1111_env_key in os.environ:\n",
    "    a_1111_base_url = os.environ[a_1111_env_key]\n",
    "\n",
    "if ollama_env_key in os.environ:\n",
    "    ollama_base_url = os.environ[ollama_env_key]\n",
    "\n",
    "# bakllava model for image to text\n",
    "image_llm = Ollama(model=\"bakllava\",\n",
    "                   base_url=ollama_base_url,\n",
    "                   num_predict=100)\n",
    "\n",
    "# llama2 model for text chat\n",
    "text_llm = ChatOllama(model=\"llama2\", base_url=ollama_base_url)\n",
    "\n",
    "# mistral 7b functioncall model for function calling\n",
    "fc_llm = ChatOllama(model=\"klcoder/mistral-7b-functioncall\",\n",
    "                    format=\"json\", num_predict=100)\n",
    "\n",
    "\n",
    "# config class for automatic 1111 image generation parameters with defaults\n",
    "class Config(BaseModel):\n",
    "    prompt: str\n",
    "    negative_prompt: str = ''\n",
    "    sampler_name: str = 'DPM++ 2M Karras'\n",
    "    checkpoint_name: str = 'dreamshaperXL_v21TurboDPMSDE'\n",
    "    batch_size: int = 1\n",
    "    steps: int = 20\n",
    "    seed: Optional[int] = None\n",
    "    cfg_scale: int = 7\n",
    "    width: int = 512\n",
    "    height: int = 512\n",
    "    denoising_strength: float = 0.7\n",
    "    enable_hr: bool = False\n",
    "    hr_scale: int = 2\n",
    "    hr_upscaler: str = '4xUltrasharp_4xUltrasharpV10'\n",
    "    hr_sampler_name: str = 'DPM++ 2M Karras'\n",
    "    send_images: bool = True\n",
    "    save_images: bool = True\n",
    "\n",
    "\n",
    "# argument schema for txt2image tool\n",
    "class Txt2ImageInput(BaseModel):\n",
    "    prompt: str = Field(\n",
    "        description=\"MidJourney style prompt for image generation\")\n",
    "\n",
    "\n",
    "# txt2image tool\n",
    "@tool(\"txt2image\", args_schema=Txt2ImageInput)\n",
    "def txt2image(prompt: str, **kwargs) -> Sequence[str]:\n",
    "    \"\"\"An image generation tool that takes in a prompt as string and returns a list of images encoded in base64 string. The prompt is transformed from simple English to a comma separate MidJourney image generation prompt.\"\"\"\n",
    "    config = Config(prompt=prompt, **kwargs)\n",
    "    if config.seed is None:\n",
    "        config.seed = int(random.normal(scale=2**32))\n",
    "    response = requests.post(a_1111_base_url + \"/sdapi/v1/txt2img\",\n",
    "                             json=config.dict()).json()\n",
    "    return response\n",
    "\n",
    "\n",
    "# argument schema for image2text tool\n",
    "class Image2TxtInput(BaseModel):\n",
    "    prompt: str = Field(description=\"Question regarding the image\")\n",
    "    image: str = Field(description=\"Base64 encoded image\")\n",
    "\n",
    "\n",
    "# image2txt tool\n",
    "@tool(\"image2txt\", args_schema=Image2TxtInput)\n",
    "def image2txt(prompt: str, image: str) -> str:\n",
    "    \"\"\"An image description tool that takes in a question about an image or a picture as a prompt and returns the answer as string\"\"\"\n",
    "    no_image_error = \"No image available within context. Upload an image or generate using prompt to describe it.\" \n",
    "    try:\n",
    "        if image is None or len(image) == 0:\n",
    "            return no_image_error\n",
    "        decoded = base64.b64decode(image)\n",
    "        del decoded\n",
    "    except Exception:\n",
    "        return no_image_error\n",
    "    bound = image_llm.bind(images=[image])\n",
    "    response: str = bound.invoke(prompt)\n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "# function to transform tool schema for function calling model\n",
    "def tool_to_definition(tool):\n",
    "    args = tool.args_schema.schema()\n",
    "    args = {arg: args[arg] for arg in args if arg != 'title'}\n",
    "    definition = {\n",
    "        'name': tool.name,\n",
    "        'description': tool.description.split(\" - \")[-1],\n",
    "        'parameters': args\n",
    "    }\n",
    "    return json.dumps(definition, indent=2)\n",
    "\n",
    "\n",
    "tools = [image2txt, txt2image]\n",
    "tool_descriptions = \"\\n\\n\".join([tool_to_definition(tool) for tool in tools])\n",
    "tool_executor = ToolExecutor(tools)\n",
    "\n",
    "\n",
    "# output format for function calling chain\n",
    "class OutputFormat(BaseModel):\n",
    "    function: Optional[str] = Field(description=\"Name of the function to call\")\n",
    "    arguments: Optional[Dict] = Field(\n",
    "        description=\"Arguments or parameters to pass to the function\")\n",
    "\n",
    "\n",
    "# prompt template for function calling chain\n",
    "fc_prompt = PromptTemplate.from_template(\"\"\"SYSTEM: You are a helpful assistant with access to the following functions. Use them if required -\n",
    "{tools}\n",
    "\n",
    "The output needs to be in the following format:\n",
    "{{\n",
    "    'name': <function name>,\n",
    "    'arguments': <arguments to pass to the function>\n",
    "}}\n",
    "\n",
    "For questions not related to image generation or not about an image, respond with an empty json object.\n",
    "\n",
    "User: {question}\n",
    "FUNCTION: \"\"\", partial_variables={\"tools\": tool_descriptions})\n",
    "\n",
    "# function calling chain\n",
    "fc_chain = fc_prompt | fc_llm | JsonOutputParser(pydantic_object=OutputFormat)\n",
    "\n",
    "# prompt template for chat chain\n",
    "text_prompt = PromptTemplate.from_template(\"\"\"You are a helpful agent. \n",
    "Respond to user questions honestly and truthfully.\n",
    "\n",
    "Human: {question}\n",
    "AI: \"\"\")\n",
    "\n",
    "# text chat chain\n",
    "text_chain = text_prompt | text_llm | StrOutputParser()\n",
    "\n",
    "\n",
    "# state definition for langgraph agent\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    image: Optional[str]\n",
    "\n",
    "\n",
    "# node function to execute tools\n",
    "def call_tool(state):\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    if isinstance(last_message, HumanMessage):\n",
    "        return {\"messages\": []}\n",
    "\n",
    "    tool_input = last_message.additional_kwargs\n",
    "\n",
    "    if \"image\" in state and state[\"image\"] is not None:\n",
    "        tool_input[\"image\"] = state[\"image\"]\n",
    "\n",
    "    action = ToolInvocation(\n",
    "        tool=last_message.name,\n",
    "        tool_input=tool_input,\n",
    "    )\n",
    "\n",
    "    response = tool_executor.invoke(action)\n",
    "    function_message = FunctionMessage(\n",
    "        content=json.dumps(response), name=action.tool)\n",
    "    return {\"messages\": [function_message]}\n",
    "\n",
    "\n",
    "# node function to call function calling model\n",
    "def call_fc_model(state):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    response = fc_chain.invoke({\"question\": last_message.content})\n",
    "    if 'name' in response and response['name'] in [tool.name for tool in tools]:\n",
    "        args = response[\"arguments\"]\n",
    "        if \"image\" in state and state[\"image\"] is not None:\n",
    "            args[\"image\"] = state[\"image\"]\n",
    "        return {\"messages\": [AIMessage(name=response[\"name\"], content=\"function\", additional_kwargs=args)]}\n",
    "    else:\n",
    "        return {\"messages\": [last_message]}\n",
    "\n",
    "\n",
    "# node function for chat model\n",
    "def call_model(state):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    response = text_chain.invoke({\"question\": last_message.content})\n",
    "    return {\"messages\": [AIMessage(content=response)]}\n",
    "\n",
    "\n",
    "# conditional logic to determine which edge to take based on last message\n",
    "def is_function_call(state):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    if isinstance(last_message, HumanMessage):\n",
    "        return \"human\"\n",
    "    if isinstance(last_message, AIMessage) and last_message.content == \"function\":\n",
    "        return \"function\"\n",
    "    else:\n",
    "        return \"end\"\n",
    "\n",
    "\n",
    "# langgraph agent graph definition\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"functions\", call_fc_model)\n",
    "workflow.add_node(\"model\", call_model)\n",
    "workflow.add_node(\"tools\", call_tool)\n",
    "workflow.set_entry_point(\"functions\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"functions\",\n",
    "    is_function_call,\n",
    "    {\n",
    "        \"human\": \"model\",\n",
    "        \"function\": \"tools\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"tools\", END)\n",
    "workflow.add_edge(\"model\", END)\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lAm3euIZFLSB",
    "outputId": "7168405b-2d0b-414a-969c-d08aa2d2184b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "import base64\n",
    "import json\n",
    "import streamlit as st\n",
    "from graph import app, ollama_base_url, a_1111_base_url\n",
    "from langchain_core.messages import AIMessage, HumanMessage, FunctionMessage, BaseMessage\n",
    "from pandas.io.common import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "st.set_page_config(page_title=\"LangChain with Automatic 1111 API\")\n",
    "st.title(\"LangChain with Automatic 1111 API\")\n",
    "\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state[\"messages\"] = [AIMessage(content=\"How can I help you?\")]\n",
    "\n",
    "if \"uploaded_file\" not in st.session_state:\n",
    "    st.session_state[\"uploaded_file\"] = None\n",
    "\n",
    "if \"image\" not in st.session_state:\n",
    "    st.session_state[\"image\"] = None\n",
    "\n",
    "for msg in st.session_state.messages:\n",
    "    if \"image\" not in msg.additional_kwargs:\n",
    "        st.chat_message(msg.type).write(msg.content)\n",
    "    else:\n",
    "        st.chat_message(msg.type).image(\n",
    "            msg.additional_kwargs[\"image\"], width=512\n",
    "        )\n",
    "        if \"params\" in msg.additional_kwargs:\n",
    "            with st.chat_message(msg.type).expander(\"Parameters\"):\n",
    "                st.code(msg.additional_kwargs[\"params\"])\n",
    "\n",
    "state = {}\n",
    "with st.sidebar:\n",
    "    st.text(f\"Ollama\\n{ollama_base_url}\")\n",
    "    st.text(f\"Automatic 1111\\n{a_1111_base_url}\")\n",
    "\n",
    "if uploaded_file := st.sidebar.file_uploader(\"Upload an image file\",\n",
    "                                             type=[\"jpg\", \"png\"]):\n",
    "    if st.session_state.uploaded_file != uploaded_file:\n",
    "        st.session_state.uploaded_file = uploaded_file\n",
    "        st.session_state.image = base64.b64encode(uploaded_file.getvalue()).decode()\n",
    "        st.session_state.messages.append(\n",
    "            HumanMessage(\n",
    "                content=uploaded_file.name,\n",
    "                additional_kwargs={\n",
    "                    \"image\": uploaded_file,\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        st.chat_message(\"user\").image(uploaded_file, width=512)\n",
    "\n",
    "if prompt := st.chat_input():\n",
    "    human_message = HumanMessage(content=prompt)\n",
    "    state[\"messages\"] = [human_message]\n",
    "    st.session_state.messages.append(human_message)\n",
    "    st.chat_message(\"human\").write(prompt)\n",
    "\n",
    "    response = \"\"\n",
    "    if st.session_state.image is not None:\n",
    "        image = st.session_state.image\n",
    "        state[\"image\"] = image\n",
    "\n",
    "    response = app.invoke(state)\n",
    "    messages = response[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    if isinstance(last_message, AIMessage):\n",
    "        st.chat_message(\"assistant\").write(last_message.content)\n",
    "        st.session_state.messages.append(last_message)\n",
    "    else:\n",
    "        if isinstance(last_message, FunctionMessage):\n",
    "            if last_message.name == \"image2txt\":\n",
    "                last_message.content = str(last_message.content).strip('\"')\n",
    "                st.chat_message(last_message.name).write(last_message.content)\n",
    "                st.session_state.messages.append(last_message)\n",
    "            elif last_message.name == \"txt2image\":\n",
    "                content = json.loads(str(last_message.content))\n",
    "                imageb64 = content[\"images\"][0]\n",
    "                params = content[\"parameters\"]\n",
    "                params = json.dumps({p: params[p] for p in params if (\n",
    "                    params[p] is not None and\n",
    "                    params[p] != 0 and\n",
    "                    params[p] is not False and\n",
    "                    params[p] != \"\" and\n",
    "                    params[p] != [] and\n",
    "                    params[p] != {}\n",
    "                )}, indent=2)\n",
    "                image = Image.open(BytesIO(base64.b64decode(imageb64)))\n",
    "                st.chat_message(last_message.name).image(image, width=512)\n",
    "                with st.chat_message(last_message.name).expander(\"Parameters\"):\n",
    "                    st.code(params)\n",
    "                st.session_state.messages.append(\n",
    "                    BaseMessage(\n",
    "                        name=last_message.name,\n",
    "                        type=last_message.name,\n",
    "                        content=\"\",\n",
    "                        additional_kwargs={\n",
    "                            \"params\": params,\n",
    "                            \"image\": image,\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "                st.session_state.image = imageb64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFa0Fpq-PMtf"
   },
   "source": [
    "### Download and run ollama\n",
    "\n",
    "Below, we:\n",
    "1. download the ollama binary\n",
    "2. make it executable\n",
    "3. start ollama in the background\n",
    "4. download the hosted bakllava, llama2, and function calling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dA6_BjFEPMtf"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!curl -L https://ollama.ai/download/ollama-linux-amd64 -o ollama\n",
    "!chmod +x ollama\n",
    "!./ollama serve &>/content/ollama_logs.txt &\n",
    "!./ollama pull bakllava\n",
    "!./ollama pull llama2\n",
    "!./ollama pull klcoder/mistral-7b-functioncall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start and background streamlit app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YeNLTTJSY5NM"
   },
   "outputs": [],
   "source": [
    "!streamlit run app.py &>/content/logs.txt &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifux-mmzcTQK"
   },
   "source": [
    "## Find the IP of your instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nQz0WaTTcTQK",
    "outputId": "99462135-a0aa-41c9-f0cf-a21e0eb144e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.227.3.82\n",
      "Copy this IP into the webpage that opens below\n"
     ]
    }
   ],
   "source": [
    "!curl ipv4.icanhazip.com\n",
    "!echo \"Copy this IP into the webpage that opens below\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1XkNhk9abV29"
   },
   "source": [
    "## Expose the Streamlit app on port 8501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_DDegT-jZ2Q5",
    "outputId": "e319b466-95b7-4e6d-e07e-a6eb3a6eb1a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[?25hnpx: installed 22 in 3.73s\n",
      "your url is: https://common-mails-doubt.loca.lt\n"
     ]
    }
   ],
   "source": [
    "!npx localtunnel --port 8501"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
