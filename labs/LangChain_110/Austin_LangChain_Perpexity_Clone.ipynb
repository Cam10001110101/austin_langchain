{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP20Nz5NniqdcGGeJCuwguu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saurabhlalsaxena/austin_langchain/blob/main/labs/LangChain_110/Austin_LangChain_Perpexity_Clone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "N6QitVkoGOCe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKwOb9OPGEtm",
        "outputId": "ed0b49e7-e092-4905-a077-2f74bd29df05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.9/109.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.4/396.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.0/290.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.0/94.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U langchain-cli langchain langchain_core langgraph langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchainhub"
      ],
      "metadata": {
        "id": "ZFQX3hqxGaTj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai langchain_openai langchain-anthropic langchain-mistralai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvOkeptcGcPv",
        "outputId": "2c1f3bf0-46a2-4f35-ef5e-808086190d02"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m833.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m705.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m891.9/891.9 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q duckduckgo-search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7jeTbiEGrSa",
        "outputId": "1219da21-dc8b-4094-bb61-1a01debbf86f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q html2text beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAZSUimcGtZi",
        "outputId": "08ace0b4-b80a-4ebb-db64-5a9d9b273af7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m51.2/56.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m993.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for html2text (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U psycopg psycopg-pool"
      ],
      "metadata": {
        "id": "cbH8Nx_fGwZI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U langgraph-checkpoint-postgres"
      ],
      "metadata": {
        "id": "1tWVfzM8GyQN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze | grep -E 'langchain|langchain_core|langgraph|langchainhub|openai|langchain_openai|langchain-anthropic|langchain-mistralai|duckduckgo-search|html2text|beautifulsoup4|psycopg|psycopg-pool|langgraph-checkpoint-postgres' > requirements.txt"
      ],
      "metadata": {
        "id": "AQqLXt7GG3ej"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = userdata.get('ANTHROPIC_API_KEY')\n",
        "os.environ[\"MISTRAL_API_KEY\"] = userdata.get('MISTRAL_API_KEY')"
      ],
      "metadata": {
        "id": "pmeTOHaEHMsA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"Austin LangChain Perplexity Clone\""
      ],
      "metadata": {
        "id": "3TsUMWl2HQYo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['USER_AGENT'] = 'myagent'"
      ],
      "metadata": {
        "id": "uHL8k87qLn3k"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LangGraph"
      ],
      "metadata": {
        "id": "rThUE4tmHuqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#prompts\n",
        "SEARCH_QUERY_PROMPT = \"\"\"You are a helpful AI assistant, create a list of 2-3 search queries based on the message\"\"\"\n",
        "FINAL_NODE_SYSTEM_PROMPT = \"\"\"You are a helpful AI assitant, answer the given question based on the context. Clearly cite the sources for your answer including the links for the sources next to the each point\"\"\"\n",
        "FINAL_NODE_PROMPT = \"\"\"Question: {question}\n",
        "Context: {context}\n",
        "Answer:\"\"\""
      ],
      "metadata": {
        "id": "MwTp9oPcHwIq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#duckduckgo utils\n",
        "import asyncio\n",
        "\n",
        "from duckduckgo_search import AsyncDDGS\n",
        "\n",
        "async def aget_results(word):\n",
        "    results = await AsyncDDGS(proxy=None).atext(word, max_results=2)\n",
        "    return results\n",
        "\n",
        "async def search(queries):\n",
        "    words = queries\n",
        "    tasks = [aget_results(w) for w in words]\n",
        "    results = await asyncio.gather(*tasks)\n",
        "    return results"
      ],
      "metadata": {
        "id": "doNmA3NPH242"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#nodes\n",
        "from langgraph.prebuilt import ToolInvocation\n",
        "import json\n",
        "from langchain_core.messages import (\n",
        "    AIMessage,\n",
        "    BaseMessage,\n",
        "    ChatMessage,\n",
        "    FunctionMessage,\n",
        "    HumanMessage,\n",
        ")\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.chains import create_structured_output_runnable\n",
        "from langchain.agents.agent_types import AgentType\n",
        "from typing import TypedDict, Annotated, Sequence, List\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "import operator\n",
        "from langchain_mistralai.chat_models import ChatMistralAI\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.constants import Send\n",
        "from langchain_community.document_loaders import AsyncHtmlLoader\n",
        "from langchain_community.document_transformers import Html2TextTransformer\n",
        "\n",
        "model = ChatMistralAI(api_key=os.environ.get('MISTRAL_API_KEY'),model=\"mistral-large-latest\",temperature=0)\n",
        "#model = ChatAnthropic(temperature=0, model_name=\"claude-3-haiku-20240307\", max_tokens=4096)\n",
        "final_model = ChatAnthropic(temperature=0, model_name=\"claude-3-5-sonnet-20240620\", max_tokens=4096)\n",
        "#model = ChatOpenAI(model=\"gpt-4o-mini\",temperature=0, streaming=True)\n",
        "\n",
        "class Queries(BaseModel):\n",
        "    \"\"\"List of search queries\"\"\"\n",
        "    queries: List[str] = Field(\n",
        "        description=\"List of the generated search queries\"\n",
        "    )\n",
        "\n",
        "class SummaryState(TypedDict):\n",
        "    content: str\n",
        "    query: str\n",
        "\n",
        "class OverallState(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
        "    search_queries: list[str]\n",
        "    search_results: list[str]\n",
        "    page_content: list[str]\n",
        "    page_summaries: Annotated[list, operator.add]\n",
        "\n",
        "    @classmethod\n",
        "    def search_query_node(cls,state):\n",
        "      messages = state['messages']\n",
        "      last_message = messages[-1]\n",
        "\n",
        "      prompt = ChatPromptTemplate.from_messages(\n",
        "            [\n",
        "                (\n",
        "                    \"system\",\n",
        "                    SEARCH_QUERY_PROMPT,\n",
        "                ),\n",
        "                MessagesPlaceholder(variable_name=\"messages\"),\n",
        "            ]\n",
        "        )\n",
        "      chain = prompt | model.with_structured_output(Queries)\n",
        "      result = chain.invoke(messages)\n",
        "      queries = result.queries\n",
        "      return {\"search_queries\":queries}\n",
        "\n",
        "    @classmethod\n",
        "    def search_results_node(cls,state):\n",
        "      queries = state['search_queries']\n",
        "      results = asyncio.run(search(queries))\n",
        "      return {\"search_results\":results}\n",
        "\n",
        "    @classmethod\n",
        "    def web_scape_node(cls,state):\n",
        "      search_results = state['search_results']\n",
        "      urls = [result['href'] for search_result in search_results for result in search_result]\n",
        "      #print(urls)\n",
        "      loader = AsyncHtmlLoader(urls)\n",
        "      docs = loader.load()\n",
        "      html2text = Html2TextTransformer()\n",
        "      docs_transformed = html2text.transform_documents(docs)\n",
        "      return {\"page_content\":docs_transformed}\n",
        "\n",
        "    @classmethod\n",
        "    def generate_summary(cls,state: SummaryState):\n",
        "      content = state['content'].page_content\n",
        "      source = state['content'].metadata['source']\n",
        "      query = state['query']\n",
        "      prompt = f\"Summarize the following content to answer the question: {query}, mention the source: {source}   \\n\\n <content> {content} </content>\"\n",
        "      page_summary = model.invoke(prompt)\n",
        "      return {\"page_summaries\":[page_summary.content]}\n",
        "\n",
        "    @classmethod\n",
        "    def continue_to_summarise_node(cls,state):\n",
        "      return [Send(\"Generate Summary\", {\"content\": p, \"query\": state['messages'][0].content}) for p in state['page_content']]\n",
        "\n",
        "    @classmethod\n",
        "    def final_result_node(cls,state):\n",
        "      messages = state['messages']\n",
        "      question = messages[-1]\n",
        "      context = state['page_summaries']\n",
        "      prompt = ChatPromptTemplate.from_messages(\n",
        "            [\n",
        "                (\n",
        "                    \"system\",\n",
        "                    FINAL_NODE_SYSTEM_PROMPT,\n",
        "                ),\n",
        "                (\n",
        "                    \"human\",\n",
        "                    FINAL_NODE_PROMPT,\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "      input = {\"question\":messages[-1],\"context\":context}\n",
        "      formatted_prompt = prompt.format_messages(**input)\n",
        "      response = final_model.invoke(formatted_prompt)\n",
        "      return {\"messages\":[response]}"
      ],
      "metadata": {
        "id": "ubBu2ZZXH5NE"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "memory = MemorySaver()"
      ],
      "metadata": {
        "id": "rsQYde64I8nA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#graph\n",
        "from langgraph.graph import StateGraph, END\n",
        "perplexity_clone = StateGraph(OverallState)\n",
        "perplexity_clone.add_node('Query Generator',OverallState.search_query_node)\n",
        "perplexity_clone.add_node('Search Results',OverallState.search_results_node)\n",
        "perplexity_clone.add_node('Web Scraper',OverallState.web_scape_node)\n",
        "perplexity_clone.add_node('Generate Summary',OverallState.generate_summary)\n",
        "perplexity_clone.add_node('Final Result',OverallState.final_result_node)\n",
        "\n",
        "perplexity_clone.set_entry_point('Query Generator')\n",
        "perplexity_clone.set_finish_point('Final Result')\n",
        "\n",
        "perplexity_clone.add_edge('Query Generator','Search Results')\n",
        "perplexity_clone.add_edge('Search Results','Web Scraper')\n",
        "perplexity_clone.add_conditional_edges('Web Scraper', OverallState.continue_to_summarise_node,['Generate Summary'])\n",
        "perplexity_clone.add_edge('Generate Summary','Final Result')\n",
        "\n",
        "perplexity_clone_graph = perplexity_clone.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "2lzwWl7GICrx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(perplexity_clone_graph.get_graph().draw_mermaid_png()))\n",
        "except Exception:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "3SgM7m03IExY",
        "outputId": "b1ee0912-6726-4a4c-bbda-d97b52813ebf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "def generate_random_alphanumeric(length=4):\n",
        "  characters = string.ascii_letters + string.digits\n",
        "  result = ''.join(random.choice(characters) for i in range(length))\n",
        "  return result\n",
        "\n",
        "query_id = generate_random_alphanumeric()\n",
        "print(query_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qABRgKReII_N",
        "outputId": "b49d2127-7c92-45ad-eeaa-d20768436d6e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "d7ZY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": f'{query_id}'},\"recursion_limit\": 20}"
      ],
      "metadata": {
        "id": "OccV_bDcIhsK"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import BaseMessage, HumanMessage\n",
        "inputs = {\n",
        "    \"messages\": [HumanMessage(content=\"Create a quick start guide for LangGraph\")],\n",
        "}"
      ],
      "metadata": {
        "id": "S9_pCTgeIr4Q"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output=perplexity_clone_graph.invoke(inputs, config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z42NtnSPIuVL",
        "outputId": "bfa4d9d3-2796-44e4-bbf7-5ba0c9dab2c3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching pages: 100%|##########| 6/6 [00:00<00:00, 20.36it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5KK7mhVIwDI",
        "outputId": "8a342af8-5ac9-49df-d43f-2c1b26d55128"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'messages': [HumanMessage(content='Create a quick start guide for LangGraph'), AIMessage(content='Here\\'s a quick start guide for LangGraph:\\n\\n1. Install LangGraph:\\n   ```\\n   pip install langgraph\\n   ```\\n   [Source: Inferred from context]\\n\\n2. Define the Graph State:\\n   ```python\\n   from typing import Dict, TypedDict, Optional\\n   class GraphState(TypedDict):\\n       question: Optional[str] = None\\n       classification: Optional[str] = None\\n       response: Optional[str] = None\\n   ```\\n   [Source: https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141]\\n\\n3. Create the Graph:\\n   ```python\\n   from langgraph.graph import StateGraph\\n   workflow = StateGraph(GraphState)\\n   ```\\n   [Source: https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141]\\n\\n4. Define Nodes:\\n   ```python\\n   def classify_input_node(state):\\n       question = state.get(\\'question\\', \\'\\').strip()\\n       classification = classify(question)  # Assume a function that classifies the input\\n       return {\"classification\": classification}\\n\\n   def handle_greeting_node(state):\\n       return {\"response\": \"Hello! How can I help you today?\"}\\n\\n   def handle_search_node(state):\\n       question = state.get(\\'question\\', \\'\\').strip()\\n       search_result = f\"Search result for \\'{question}\\'\"\\n       return {\"response\": search_result}\\n   ```\\n   [Source: https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141]\\n\\n5. Add Nodes to the Graph:\\n   ```python\\n   workflow.add_node(\"classify_input\", classify_input_node)\\n   workflow.add_node(\"handle_greeting\", handle_greeting_node)\\n   workflow.add_node(\"handle_search\", handle_search_node)\\n\\n   def decide_next_node(state):\\n       return \"handle_greeting\" if state.get(\\'classification\\') == \"greeting\" else \"handle_search\"\\n\\n   workflow.add_conditional_edges(\\n       \"classify_input\",\\n       decide_next_node,\\n       {\\n           \"handle_greeting\": \"handle_greeting\",\\n           \"handle_search\": \"handle_search\"\\n       }\\n   )\\n   ```\\n   [Source: https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141]\\n\\n6. Set Entry and End Points:\\n   ```python\\n   workflow.set_entry_point(\"classify_input\")\\n   workflow.add_edge(\\'handle_greeting\\', END)\\n   workflow.add_edge(\\'handle_search\\', END)\\n   ```\\n   [Source: https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141]\\n\\n7. Compile and Run the Graph:\\n   ```python\\n   app = workflow.compile()\\n   inputs = {\"question\": \"Hello, how are you?\"}\\n   result = app.invoke(inputs)\\n   print(result)\\n   ```\\n   [Source: https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141]\\n\\n8. For more advanced use cases and tutorials, visit the official LangGraph Tutorials page: [https://langchain-ai.github.io/langgraph/tutorials/]\\n\\nKey Concepts to Remember:\\n- Stateful Graph: The graph maintains a state that is passed around and updated as the computation progresses.\\n- Nodes: Represent functions or computation steps in your graph.\\n- Edges: Connect nodes and define the flow of computation, including conditional edges.\\n[Source: https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141]\\n\\nThis quick start guide should help you get started with LangGraph and create a simple application. For more complex use cases and detailed tutorials, refer to the official documentation and tutorials.', response_metadata={'id': 'msg_01S5tYKjXpCbYaYFo4RELEfw', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 4808, 'output_tokens': 960}}, id='run-8d0c25cd-d656-4e30-a0dd-de9fc71d97a7-0', usage_metadata={'input_tokens': 4808, 'output_tokens': 960, 'total_tokens': 5768})], 'search_queries': ['How to create a quick start guide for LangGraph', 'LangGraph quick start guide', 'Tutorial for LangGraph quick start guide'], 'search_results': [[{'title': \"Introduction to LangGraph: A Beginner's Guide - Medium\", 'href': 'https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141', 'body': \"Step 1: Define the Graph State. First, we define the state structure for our graph. In this example, our state includes the user's question, the classification of the question, and a response ...\"}, {'title': 'LangGraph Tutorial: What Is LangGraph and How to Use It?', 'href': 'https://www.datacamp.com/tutorial/langgraph-tutorial', 'body': 'LangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner. It simplifies the development process by enabling the creation of cyclical graphs, which are essential for developing ...'}], [{'title': 'Tutorials - GitHub Pages', 'href': 'https://langchain-ai.github.io/langgraph/tutorials/', 'body': 'Quick Start¶ Learn the basics of LangGraph through a comprehensive quick start in which you will build an agent from scratch. ... Reflexion: Critique missing and superfluous details to guide next steps; Language Agent Tree Search: Use reflection and rewards to drive a tree search over agents;'}, {'title': 'Quick Start - GitHub Pages', 'href': 'https://langchain-ai.github.io/langgraph/cloud/quick_start/', 'body': 'Quick Start. This quick start guide will cover how to build a simple agent that can look up things on the internet. We will then deploy it to LangGraph Cloud, use the LangGraph Studio to visualize and test it out, and use the LangGraph SDK to interact with it.'}], [{'title': 'Tutorials - GitHub Pages', 'href': 'https://langchain-ai.github.io/langgraph/tutorials/', 'body': 'Welcome to the LangGraph Tutorials! These notebooks introduce LangGraph through building various language agents and applications. ... Quick Start¶ Learn the basics of LangGraph through a comprehensive quick start in which you will build an agent from scratch. ... Reflexion: Critique missing and superfluous details to guide next steps ...'}, {'title': \"Introduction to LangGraph: A Beginner's Guide - Medium\", 'href': 'https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141', 'body': \"Step 1: Define the Graph State. First, we define the state structure for our graph. In this example, our state includes the user's question, the classification of the question, and a response ...\"}]], 'page_content': [Document(metadata={'source': 'https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141', 'title': \"Mastering LangGraph: A Beginner's Guide to Building Intelligent Language Models | Medium\", 'description': \"Unlock the power of LangGraph with our beginner's guide. Learn to build stateful applications with LLMs and enhance your AI projects with expert tips. Master LangGraph now!\", 'language': 'en'}, page_content='Open in app\\n\\nSign up\\n\\nSign in\\n\\nWrite\\n\\nSign up\\n\\nSign in\\n\\n# Introduction to LangGraph: A Beginner’s Guide\\n\\nCPlog\\n\\n·\\n\\nFollow\\n\\n3 min read\\n\\n·\\n\\nFeb 15, 2024\\n\\n\\\\--\\n\\n3\\n\\nListen\\n\\nShare\\n\\nLangGraph is a powerful tool for building stateful, multi-actor applications\\nwith Large Language Models (LLMs). It extends the LangChain library, allowing\\nyou to coordinate multiple chains (or actors) across multiple steps of\\ncomputation in a cyclic manner. In this article, we’ll introduce LangGraph,\\nwalk you through its basic concepts, and share some insights and common points\\nof confusion for beginners.\\n\\n# What is LangGraph?\\n\\nLangGraph is a library built on top of LangChain, designed to add cyclic\\ncomputational capabilities to your LLM applications. While LangChain allows\\nyou to define chains of computation (Directed Acyclic Graphs or DAGs),\\nLangGraph introduces the ability to add cycles, enabling more complex, agent-\\nlike behaviors where you can call an LLM in a loop, asking it what action to\\ntake next.\\n\\n# Key Concepts\\n\\n  * **Stateful Graph:** LangGraph revolves around the concept of a stateful graph, where each node in the graph represents a step in your computation, and the graph maintains a state that is passed around and updated as the computation progresses.\\n  * **Nodes:** Nodes are the building blocks of your LangGraph. Each node represents a function or a computation step. You define nodes to perform specific tasks, such as processing input, making decisions, or interacting with external APIs.\\n  * **Edges:** Edges connect the nodes in your graph, defining the flow of computation. LangGraph supports conditional edges, allowing you to dynamically determine the next node to execute based on the current state of the graph.\\n\\n# A Simple Example\\n\\nLet’s walk through a simple example where we use LangGraph to classify user\\ninput as either a “greeting” or a “search” query and respond accordingly.\\n\\n# Step 1: Define the Graph State\\n\\nFirst, we define the state structure for our graph. In this example, our state\\nincludes the user’s question, the classification of the question, and a\\nresponse.\\n\\n    \\n    \\n    from typing import Dict, TypedDict, Optional  \\n    class GraphState(TypedDict):  \\n        question: Optional[str] = None  \\n        classification: Optional[str] = None  \\n        response: Optional[str] = None\\n\\n# Step 2: Create the Graph\\n\\nNext, we create a new instance of `StateGraph` with our `GraphState`\\nstructure.\\n\\n    \\n    \\n    from langgraph.graph import StateGraph  \\n    workflow = StateGraph(GraphState)\\n\\n# Step 3: Define Nodes\\n\\nWe define nodes for classifying the input, handling greetings, and handling\\nsearch queries.\\n\\n    \\n    \\n    def classify_input_node(state):  \\n        question = state.get(\\'question\\', \\'\\').strip()  \\n        classification = classify(question)  # Assume a function that classifies the input  \\n        return {\"classification\": classification}  \\n    def handle_greeting_node(state):  \\n        return {\"response\": \"Hello! How can I help you today?\"}  \\n    def handle_search_node(state):  \\n        question = state.get(\\'question\\', \\'\\').strip()  \\n        search_result = f\"Search result for \\'{question}\\'\"  \\n        return {\"response\": search_result}\\n\\n# Step 4: Add Nodes to the Graph\\n\\nWe add our nodes to the graph and define the flow using edges and conditional\\nedges.\\n\\n    \\n    \\n    workflow.add_node(\"classify_input\", classify_input_node)  \\n    workflow.add_node(\"handle_greeting\", handle_greeting_node)  \\n    workflow.add_node(\"handle_search\", handle_search_node)  \\n      \\n    def decide_next_node(state):  \\n        return \"handle_greeting\" if state.get(\\'classification\\') == \"greeting\" else \"handle_search\"  \\n    workflow.add_conditional_edges(  \\n        \"classify_input\",  \\n        decide_next_node,  \\n        {  \\n            \"handle_greeting\": \"handle_greeting\",  \\n            \"handle_search\": \"handle_search\"  \\n        }  \\n    )\\n\\n# Step 5: Set Entry and End Points\\n\\nWe set the entry point for our graph and define the end points.\\n\\n    \\n    \\n    workflow.set_entry_point(\"classify_input\")  \\n    workflow.add_edge(\\'handle_greeting\\', END)  \\n    workflow.add_edge(\\'handle_search\\', END)\\n\\n# Step 6: Compile and Run the Graph\\n\\nFinally, we compile our graph and run it with some input.\\n\\n    \\n    \\n    app = workflow.compile()  \\n    inputs = {\"question\": \"Hello, how are you?\"}  \\n    result = app.invoke(inputs)  \\n    print(result)\\n\\n# Common Confusions\\n\\n  * **State Management:** Understanding how the state is passed around and updated in the graph can be tricky. Remember that each node receives the current state, can modify it, and passes it on to the next node.\\n  * **Conditional Edges:** Setting up conditional edges requires careful consideration of the conditions and the mapping of outcomes to the next nodes. Ensure that the keys returned by the condition function match the keys in the conditional edge mapping.\\n  * **Dead-End Nodes:** Every node in the graph should have a path leading to another node or to the `END` node. If a node has no outgoing edges, it\\'s considered a dead-end, and you\\'ll need to add an edge to avoid errors.\\n\\n# Conclusion\\n\\nLangGraph is a versatile tool for building complex, stateful applications with\\nLLMs. By understanding its core concepts and working through simple examples,\\nbeginners can start to leverage its power for their projects. Remember to pay\\nattention to state management, conditional edges, and ensuring there are no\\ndead-end nodes in your graph. Happy coding!\\n\\nLangchain\\n\\nLlm\\n\\nAI\\n\\nChatbots\\n\\nPython\\n\\n\\\\--\\n\\n\\\\--\\n\\n3\\n\\nFollow\\n\\n## Written by CPlog\\n\\n48 Followers\\n\\nAI enthusiast & Principal Data Scientist at LFX & Li & Fung. I innovate with\\nLLMs, LangChain, Stable Diffusion & ComfyUI to revolutionize supply chains.\\n\\nFollow\\n\\nHelp\\n\\nStatus\\n\\nAbout\\n\\nCareers\\n\\nPress\\n\\nBlog\\n\\nPrivacy\\n\\nTerms\\n\\nText to speech\\n\\nTeams\\n\\n'), Document(metadata={'source': 'https://www.datacamp.com/tutorial/langgraph-tutorial', 'title': 'Just a moment...', 'language': 'en-US'}, page_content='Enable JavaScript and cookies to continue\\n\\n'), Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/', 'title': 'Tutorials', 'description': 'Build language agents as graphs', 'language': 'en'}, page_content='Skip to content\\n\\nTutorials\\n\\nInitializing search\\n\\nGitHub\\n\\n  * Home \\n  * Tutorials \\n  * How-to Guides \\n  * Conceptual Guides \\n  * Reference \\n  * Cloud (beta) \\n\\nGitHub\\n\\n  * Home \\n  * Tutorials \\n\\nTutorials\\n\\n    * Quick Start \\n    * Chatbots \\n    * RAG \\n    * Agent Architectures \\n    * Evaluation & Analysis \\n    * Experimental \\n  * How-to Guides \\n  * Conceptual Guides \\n  * Reference \\n  * Cloud (beta) \\n\\nTable of contents\\n\\n  * Quick Start \\n  * Use cases \\n    * Chatbots \\n    * Multi-Agent Systems \\n    * RAG \\n    * Planning Agents \\n    * Reflection & Critique \\n    * Evaluation \\n    * Experimental \\n\\n# Tutorials¶\\n\\nWelcome to the LangGraph Tutorials! These notebooks introduce LangGraph\\nthrough building various language agents and applications.\\n\\n## Quick Start¶\\n\\nLearn the basics of LangGraph through a comprehensive quick start in which you\\nwill build an agent from scratch.\\n\\n  * Quick Start\\n\\n## Use cases¶\\n\\nLearn from example implementations of graphs designed for specific scenarios\\nand that implement common design patterns.\\n\\n#### Chatbots¶\\n\\n  * Customer Support: Build a customer support chatbot to manage flights, hotel reservations, car rentals, and other tasks\\n  * Prompt Generation from User Requirements: Build an information gathering chatbot\\n  * Code Assistant: Build a code analysis and generation assistant\\n\\n#### Multi-Agent Systems¶\\n\\n  * Collaboration: Enable two agents to collaborate on a task\\n  * Supervision: Use an LLM to orchestrate and delegate to individual agents\\n  * Hierarchical Teams: Orchestrate nested teams of agents to solve problems\\n\\n#### RAG¶\\n\\n  * Adaptive RAG\\n    * Adaptive RAG using local LLMs\\n  * Agentic RAG\\n  * Corrective RAG\\n    * Corrective RAG using local LLMs\\n  * Self-RAG\\n    * Self-RAG using local LLMs\\n  * SQL Agent\\n\\n#### Planning Agents¶\\n\\n  * Plan-and-Execute: Implement a basic planning and execution agent\\n  * Reasoning without Observation: Reduce re-planning by saving observations as variables\\n  * LLMCompiler: Stream and eagerly execute a DAG of tasks from a planner\\n\\n#### Reflection & Critique¶\\n\\n  * Basic Reflection: Prompt the agent to reflect on and revise its outputs\\n  * Reflexion: Critique missing and superfluous details to guide next steps\\n  * Language Agent Tree Search: Use reflection and rewards to drive a tree search over agents\\n  * Self-Discover Agent: Analyze an agent that learns about its own capabilities\\n\\n#### Evaluation¶\\n\\n  * Agent-based: Evaluate chatbots via simulated user interactions\\n  * In LangSmith: Evaluate chatbots in LangSmith over a dialog dataset\\n\\n#### Experimental¶\\n\\n  * Web Research (STORM): Generate Wikipedia-like articles via research and multi-perspective QA\\n  * TNT-LLM: Build rich, interpretable taxonomies of user intentand using the classification system developed by Microsoft for their Bing Copilot application.\\n  * Web Navigation: Build an agent that can navigate and interact with websites\\n  * Competitive Programming: Build an agent with few-shot \"episodic memory\" and human-in-the-loop collaboration to solve problems from the USA Computing Olympiad; adapted from the \"Can Language Models Solve Olympiad Programming?\" paper by Shi, Tang, Narasimhan, and Yao.\\n  * Complex data extraction: Build an agent that can use function calling to do complex extraction tasks\\n\\n## Comments\\n\\nBack to top\\n\\nPrevious\\n\\n🦜🕸️LangGraph\\n\\nNext\\n\\nQuick Start\\n\\nMade with  Material for MkDocs\\n\\n'), Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/cloud/quick_start/', 'title': 'Quick Start', 'description': 'Build language agents as graphs', 'language': 'en'}, page_content='Skip to content\\n\\nQuick Start\\n\\nInitializing search\\n\\nGitHub\\n\\n  * Home \\n  * Tutorials \\n  * How-to Guides \\n  * Conceptual Guides \\n  * Reference \\n  * Cloud (beta) \\n\\nGitHub\\n\\n  * Home \\n  * Tutorials \\n  * How-to Guides \\n  * Conceptual Guides \\n  * Reference \\n  * Cloud (beta) \\n\\nCloud (beta)\\n\\n    * Tutorials  Tutorials \\n      * Quick Start  Quick Start  Table of contents \\n        * Set up requirements \\n        * Set up local files \\n        * Test the graph build locally \\n          * Using LangGraph Studio Desktop (recommended) \\n          * Using the LangGraph CLI \\n        * Deploy to Cloud \\n          * Push your code to GitHub \\n          * Deploy from GitHub with LangGraph Cloud \\n        * Inspect Traces + Monitor Service \\n          * Deployments View \\n          * Access the Docs \\n        * Interact with your deployment via LangGraph Studio \\n        * Use with the SDK \\n        * What\\'s Next \\n          * LangGraph Cloud How-tos \\n          * LangGraph Tutorials \\n    * How-to Guides \\n    * Conceptual Guides \\n    * Reference \\n    * FAQ \\n\\nTable of contents\\n\\n  * Set up requirements \\n  * Set up local files \\n  * Test the graph build locally \\n    * Using LangGraph Studio Desktop (recommended) \\n    * Using the LangGraph CLI \\n  * Deploy to Cloud \\n    * Push your code to GitHub \\n    * Deploy from GitHub with LangGraph Cloud \\n  * Inspect Traces + Monitor Service \\n    * Deployments View \\n    * Access the Docs \\n  * Interact with your deployment via LangGraph Studio \\n  * Use with the SDK \\n  * What\\'s Next \\n    * LangGraph Cloud How-tos \\n    * LangGraph Tutorials \\n\\n# Quick Start¶\\n\\nThis quick start guide will cover how to build a simple agent that can look up\\nthings on the internet. We will then deploy it to LangGraph Cloud, use the\\nLangGraph Studio to visualize and test it out, and use the LangGraph SDK to\\ninteract with it.\\n\\n## Set up requirements¶\\n\\nThis tutorial will use:\\n\\n  * Anthropic for the LLM - sign up and get an API key here\\n  * Tavily for the search engine - sign up and get an API key here\\n  * LangSmith for hosting - sign up and get an API key here\\n\\n## Set up local files¶\\n\\n  1. Create a new application with the following directory and files:\\n\\nPythonJavascript\\n\\n    \\n    \\n    <my-app>/\\n    |-- agent.py            # code for your LangGraph agent\\n    |-- requirements.txt    # Python packages required for your graph\\n    |-- langgraph.json      # configuration file for LangGraph\\n    |-- .env                # environment files with API keys\\n    \\n    \\n    \\n    <my-app>/\\n    |-- agent.ts            # code for your LangGraph agent\\n    |-- package.json        # Javascript packages required for your graph\\n    |-- langgraph.json      # configuration file for LangGraph\\n    |-- .env                # environment files with API keys\\n    \\n\\n  1. The `agent.py`/`agent.ts` file should contain code for defining your graph. The following code is a simple example, the important thing is that at some point in your file you compile your graph and assign the compiled graph to a variable (in this case the `graph` variable). This example code uses `create_react_agent`, a prebuilt agent. You can read more about it here.\\n\\nPythonJavascript\\n\\n    \\n    \\n    from langchain_anthropic import ChatAnthropic\\n    from langchain_community.tools.tavily_search import TavilySearchResults\\n    from langgraph.prebuilt import create_react_agent\\n    \\n    model = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\\n    \\n    tools = [TavilySearchResults(max_results=2)]\\n    \\n    graph = create_react_agent(model, tools)\\n    \\n    \\n    \\n    import { ChatAnthropic } from \"@langchain/anthropic\";\\n    import { TavilySearchResults } from \"@langchain/community/tools/tavily_search\";\\n    import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\\n    \\n    const model = new ChatAnthropic({\\n      model: \"claude-3-5-sonnet-20240620\",\\n    });\\n    \\n    const tools = [\\n      new TavilySearchResults({ maxResults: 3, }),\\n    ];\\n    \\n    export const graph = createReactAgent({ llm: model, tools });\\n    \\n\\n  1. The `requirements.txt`/`package.json` file should contain any dependencies for your graph(s). In this case we only require four packages for our graph to run:\\n\\nPythonJavascript\\n\\n    \\n    \\n    langgraph\\n    langchain_anthropic\\n    tavily-python\\n    langchain_community\\n    \\n    \\n    \\n    {\\n      \"name\": \"my-app\",\\n      \"packageManager\": \"yarn@1.22.22\",\\n      \"dependencies\": {\\n        \"@langchain/community\": \"^0.2.31\",\\n        \"@langchain/core\": \"^0.2.31\",\\n        \"@langchain/langgraph\": \"0.2.0\",\\n        \"@langchain/openai\": \"^0.2.8\"\\n      }\\n    }\\n    \\n\\n  1. The `langgraph.json` file is a configuration file that describes what graph(s) you are going to host. In this case we only have one graph to host: the compiled `graph` object from `agent.py`/`agent.ts`.\\n\\nPythonJavascript\\n\\n    \\n    \\n    {\\n      \"dependencies\": [\".\"],\\n      \"graphs\": {\\n        \"agent\": \"./agent.py:graph\"\\n      },\\n      \"env\": \".env\"\\n    }\\n    \\n    \\n    \\n    {\\n      \"node_version\": \"20\",\\n      \"dockerfile_lines\": [],\\n      \"dependencies\": [\".\"],\\n      \"graphs\": {\\n        \"agent\": \"./src/agent.ts:graph\"\\n      },\\n      \"env\": \".env\"\\n    }\\n    \\n\\nLearn more about the LangGraph CLI configuration file here.\\n\\n  1. The `.env` file should have any environment variables needed to run your graph. This will only be used for local testing, so if you are not testing locally you can skip this step. NOTE: if you do add this, you should NOT check this into git. For this graph, we need two environment variables:\\n    \\n        ANTHROPIC_API_KEY=...\\n    TAVILY_API_KEY=...\\n    \\n\\nNow that we have set everything up on our local file system, we are ready to\\nhost our graph.\\n\\n## Test the graph build locally¶\\n\\n### Using LangGraph Studio Desktop (recommended)¶\\n\\nTesting your graph locally is easy with LangGraph Studio Desktop. LangGraph\\nStudio offers a new way to develop LLM applications by providing a specialized\\nagent IDE that enables visualization, interaction, and debugging of complex\\nagentic applications\\n\\nWith visual graphs and the ability to edit state, you can better understand\\nagent workflows and iterate faster. LangGraph Studio integrates with LangSmith\\nso you can collaborate with teammates to debug failure modes.\\n\\n### Using the LangGraph CLI¶\\n\\nBefore deploying to the cloud, we probably want to test the building of our\\ngraph locally. This is useful to make sure we have configured our CLI\\nconfiguration file correctly and our graph runs.\\n\\nIn order to do this we can first install the LangGraph CLI\\n\\n    \\n    \\n    pip install langgraph-cli\\n    \\n\\nWe can then test our API server locally. This requires access to LangGraph\\nclosed beta. In order to run the server locally, you will need to add your\\n`LANGSMITH_API_KEY` to the .env file so we can validate you have access to\\nLangGraph closed beta.\\n\\n    \\n    \\n    langgraph up\\n    \\n\\nThis will start up the LangGraph API server locally. If this runs\\nsuccessfully, you should see something like:\\n\\n    \\n    \\n    Ready!\\n    - API: http://localhost:8123\\n    2024-06-26 19:20:41,056:INFO:uvicorn.access 127.0.0.1:44138 - \"GET /ok HTTP/1.1\" 200\\n    \\n\\nYou can now test this out! **Note: this local server is intended SOLELY for\\nlocal testing purposes and is not performant enough for production\\napplications, so please do not use it as such.** To test it out, you can go to\\nanother terminal window and run:\\n\\n    \\n    \\n    curl --request POST \\\\\\n        --url http://localhost:8123/runs/stream \\\\\\n        --header \\'Content-Type: application/json\\' \\\\\\n        --data \\'{\\n        \"assistant_id\": \"agent\",\\n        \"input\": {\\n            \"messages\": [\\n                {\\n                    \"role\": \"user\",\\n                    \"content\": \"How are you?\"\\n                }\\n            ]\\n        },\\n        \"metadata\": {},\\n        \"config\": {\\n            \"configurable\": {}\\n        },\\n        \"multitask_strategy\": \"reject\",\\n        \"stream_mode\": [\\n            \"values\"\\n        ]\\n    }\\'\\n    \\n\\nIf you get back a valid response, then all is functioning properly!\\n\\n## Deploy to Cloud¶\\n\\n### Push your code to GitHub¶\\n\\nTurn the `<my-app>` directory into a GitHub repo. You can use the GitHub CLI\\nif you like, or just create a repo manually (if unfamiliar, instructions\\nhere).\\n\\n### Deploy from GitHub with LangGraph Cloud¶\\n\\nOnce you have created your github repository with a Python file containing\\nyour compiled graph as well as a `langgraph.json` file containing the\\nconfiguration for hosting your graph, you can head over to LangSmith and click\\non the 🚀 icon on the left navbar to create a new deployment. Then click the `+\\nNew Deployment` button.\\n\\n**_If you have not deployed to LangGraph Cloud before:_** there will be a\\nbutton that shows up saying Import from GitHub. You’ll need to follow that\\nflow to connect LangGraph Cloud to GitHub.\\n\\n**_Once you have set up your GitHub connection:_** the new deployment page\\nwill look as follows:\\n\\nTo deploy your application, you should do the following:\\n\\n  1. Select your GitHub username or organization from the selector\\n  2. Search for your repo to deploy in the search bar and select it\\n  3. Choose any name\\n  4. In the `LangGraph API config file` field, enter the path to your `langgraph.json` file (which in this case is just `langgraph.json`)\\n  5. For Git Reference, you can select either the git branch for the code you want to deploy, or the exact commit SHA.\\n  6. If your chain relies on environment variables, add those in. They will be propagated to the underlying server so your code can access them. In this case, we need `ANTHROPIC_API_KEY` and `TAVILY_API_KEY`.\\n\\nPutting this all together, you should have something as follows for your\\ndeployment details:\\n\\nHit `Submit` and your application will start deploying!\\n\\n## Inspect Traces + Monitor Service¶\\n\\n### Deployments View¶\\n\\nAfter your deployment is complete, your deployments page should look as\\nfollows:\\n\\nYou can see that by default, you get access to the `Trace Count` monitoring\\nchart and `Recent Traces` run view. These are powered by LangSmith.\\n\\nYou can click on `All Charts` to view all monitoring info for your server, or\\nclick on `See tracing project` to get more information on an individual trace.\\n\\n### Access the Docs¶\\n\\nYou can access the docs by clicking on the API docs link, which should send\\nyou to a page that looks like this:\\n\\nYou won’t actually be able to test any of the API endpoints without\\nauthorizing first. To do so, grab your Langsmith API key and add it at the top\\nwhere it says `API KEY (X-API-KEY)`. You should now be able to select any of\\nthe API endpoints, click `Test Request`, enter the parameters you would like\\nto pass, and then click `Send` to view the results of the API call.\\n\\n## Interact with your deployment via LangGraph Studio¶\\n\\nIf you click on your deployment you should see a blue button in the top right\\nthat says `LangGraph Studio`. Clicking on this button will take you to a page\\nthat looks like this:\\n\\nOn this page you can test out your graph by passing in starting states and\\nclicking `Start Run` (this should behave identically to calling `.invoke`).\\nYou will then be able to look into the execution thread for each run and\\nexplore the steps your graph is taking to produce its output.\\n\\n## Use with the SDK¶\\n\\nOnce you have tested that your hosted graph works as expected using LangGraph\\nStudio, you can start using your hosted graph all over your organization by\\nusing the LangGraph SDK. Let\\'s see how we can access our hosted graph and\\nexecute our run from a python file.\\n\\nFirst, make sure you have the SDK installed by calling `pip install\\nlanggraph_sdk`.\\n\\nBefore using, you need to get the URL of your LangGraph deployment. You can\\nfind this in the `Deployment` view. Click the URL to copy it to the clipboard.\\n\\nYou also need to make sure you have set up your API key properly so you can\\nauthenticate with LangGraph Cloud.\\n\\n    \\n    \\n    export LANGSMITH_API_KEY=...\\n    \\n\\nThe first thing to do when using the SDK is to setup our client, access our\\nassistant, and create a thread to execute a run on:\\n\\nPythonJavascriptCURL\\n\\n    \\n    \\n    from langgraph_sdk import get_client\\n    \\n    client = get_client(url=<DEPLOYMENT_URL>)\\n    # get default assistant\\n    assistants = await client.assistants.search()\\n    assistant = [a for a in assistants if not a[\"config\"]][0]\\n    # create thread\\n    thread = await client.threads.create()\\n    print(thread)\\n    \\n    \\n    \\n    import { Client } from \"@langchain/langgraph-sdk\";\\n    \\n    const client = new Client({ apiUrl: <DEPLOYMENT_URL> });\\n    // get default assistant\\n    const assistants = await client.assistants.search();\\n    const assistant = assistants.find(a => !a.config);\\n    // create thread\\n    const thread = await client.threads.create();\\n    console.log(thread)\\n    \\n    \\n    \\n    curl --request POST \\\\\\n        --url <DEPLOYMENT_URL>/assistants/search \\\\\\n        --header \\'Content-Type: application/json\\' \\\\\\n        --data \\'{\\n            \"limit\": 10,\\n            \"offset\": 0\\n        }\\' | jq -c \\'map(select(.config == null or .config == {})) | .[0]\\' && \\\\\\n    curl --request POST \\\\\\n        --url <DEPLOYMENT_URL>/threads \\\\\\n        --header \\'Content-Type: application/json\\' \\\\\\n        --data \\'{}\\'\\n    \\n\\nWe can then execute a run on the thread:\\n\\nPythonJavascriptCURL\\n\\n    \\n    \\n    input = {\"messages\":[{\"role\": \"user\", \"content\": \"Hello! My name is Bagatur and I am 26 years old.\"}]}\\n    \\n    async for chunk in client.runs.stream(\\n            thread[\\'thread_id\\'],\\n            assistant[\"assistant_id\"],\\n            input=input,\\n            stream_mode=\"updates\",\\n        ):\\n        if chunk.data and chunk.event != \"metadata\":\\n            print(chunk.data)\\n    \\n    \\n    \\n    const input = { \"messages\":[{ \"role\": \"user\", \"content\": \"Hello! My name is Bagatur and I am 26 years old.\" }] };\\n    \\n    const streamResponse = client.runs.stream(\\n      thread[\"thread_id\"],\\n      assistant[\"assistant_id\"],\\n      {\\n        input,\\n      }\\n    );\\n    for await (const chunk of streamResponse) {\\n      if (chunk.data && chunk.event !== \"metadata\" ) {\\n        console.log(chunk.data);\\n      }\\n    }\\n    \\n    \\n    \\n    curl --request POST \\\\\\n      --url <DEPLOYMENT_URL>/threads/<THREAD_ID>/runs/stream \\\\\\n      --header \\'Content-Type: application/json\\' \\\\\\n      --data \"{\\n        \\\\\"assistant_id\\\\\": <ASSISTANT_ID>,\\n        \\\\\"input\\\\\": {\\\\\"messages\\\\\": [{\\\\\"role\\\\\": \\\\\"human\\\\\", \\\\\"content\\\\\": \\\\\"Hello! My name is Bagatur and I am 26 years old.\\\\\"}]},\\n      }\" | sed \\'s/\\\\r$//\\' | awk \\'\\n      /^event:/ { event = $2 }\\n      /^data:/ {\\n          json_data = substr($0, index($0, $2))\\n    \\n          if (event != \"metadata\") {\\n          print json_data\\n          }\\n      }\\'\\n    \\n\\nOutput:\\n\\n    \\n    \\n    {\\'agent\\': {\\'messages\\': [{\\'content\\': \"Hi Bagatur! It\\'s nice to meet you. How can I assist you today?\", \\'additional_kwargs\\': {}, \\'response_metadata\\': {\\'finish_reason\\': \\'stop\\', \\'model_name\\': \\'gpt-4o-2024-05-13\\', \\'system_fingerprint\\': \\'fp_9cb5d38cf7\\'}, \\'type\\': \\'ai\\', \\'name\\': None, \\'id\\': \\'run-c89118b7-1b1e-42b9-a85d-c43fe99881cd\\', \\'example\\': False, \\'tool_calls\\': [], \\'invalid_tool_calls\\': [], \\'usage_metadata\\': None}]}}\\n    \\n\\n## What\\'s Next¶\\n\\nCongratulations! If you\\'ve worked your way through this tutorial you are well\\non your way to becoming a LangGraph Cloud expert. Here are some other\\nresources to check out to help you out on the path to expertise:\\n\\n### LangGraph Cloud How-tos¶\\n\\nIf you want to learn more about streaming from hosted graphs, check out the\\nStreaming how-to guides.\\n\\nTo learn more about double-texting and all the ways you can handle it in your\\napplication, read up on these how-to guides.\\n\\nTo learn about how to include different human-in-the-loop behavior in your\\ngraph, take a look at these how-tos.\\n\\n### LangGraph Tutorials¶\\n\\nBefore hosting, you have to write a graph to host. Here are some tutorials to\\nget you more comfortable with writing LangGraph graphs and give you\\ninspiration for the types of graphs you want to host.\\n\\nThis tutorial walks you through how to write a customer support bot using\\nLangGraph.\\n\\nIf you are interested in writing a SQL agent, check out this tutorial.\\n\\nCheck out the LangGraph tutorials page to read about more exciting use cases.\\n\\n## Comments\\n\\nBack to top\\n\\nPrevious\\n\\nLangGraph Cloud (beta)\\n\\nNext\\n\\nHow-to Guides\\n\\nMade with  Material for MkDocs\\n\\n'), Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/', 'title': 'Tutorials', 'description': 'Build language agents as graphs', 'language': 'en'}, page_content='Skip to content\\n\\nTutorials\\n\\nInitializing search\\n\\nGitHub\\n\\n  * Home \\n  * Tutorials \\n  * How-to Guides \\n  * Conceptual Guides \\n  * Reference \\n  * Cloud (beta) \\n\\nGitHub\\n\\n  * Home \\n  * Tutorials \\n\\nTutorials\\n\\n    * Quick Start \\n    * Chatbots \\n    * RAG \\n    * Agent Architectures \\n    * Evaluation & Analysis \\n    * Experimental \\n  * How-to Guides \\n  * Conceptual Guides \\n  * Reference \\n  * Cloud (beta) \\n\\nTable of contents\\n\\n  * Quick Start \\n  * Use cases \\n    * Chatbots \\n    * Multi-Agent Systems \\n    * RAG \\n    * Planning Agents \\n    * Reflection & Critique \\n    * Evaluation \\n    * Experimental \\n\\n# Tutorials¶\\n\\nWelcome to the LangGraph Tutorials! These notebooks introduce LangGraph\\nthrough building various language agents and applications.\\n\\n## Quick Start¶\\n\\nLearn the basics of LangGraph through a comprehensive quick start in which you\\nwill build an agent from scratch.\\n\\n  * Quick Start\\n\\n## Use cases¶\\n\\nLearn from example implementations of graphs designed for specific scenarios\\nand that implement common design patterns.\\n\\n#### Chatbots¶\\n\\n  * Customer Support: Build a customer support chatbot to manage flights, hotel reservations, car rentals, and other tasks\\n  * Prompt Generation from User Requirements: Build an information gathering chatbot\\n  * Code Assistant: Build a code analysis and generation assistant\\n\\n#### Multi-Agent Systems¶\\n\\n  * Collaboration: Enable two agents to collaborate on a task\\n  * Supervision: Use an LLM to orchestrate and delegate to individual agents\\n  * Hierarchical Teams: Orchestrate nested teams of agents to solve problems\\n\\n#### RAG¶\\n\\n  * Adaptive RAG\\n    * Adaptive RAG using local LLMs\\n  * Agentic RAG\\n  * Corrective RAG\\n    * Corrective RAG using local LLMs\\n  * Self-RAG\\n    * Self-RAG using local LLMs\\n  * SQL Agent\\n\\n#### Planning Agents¶\\n\\n  * Plan-and-Execute: Implement a basic planning and execution agent\\n  * Reasoning without Observation: Reduce re-planning by saving observations as variables\\n  * LLMCompiler: Stream and eagerly execute a DAG of tasks from a planner\\n\\n#### Reflection & Critique¶\\n\\n  * Basic Reflection: Prompt the agent to reflect on and revise its outputs\\n  * Reflexion: Critique missing and superfluous details to guide next steps\\n  * Language Agent Tree Search: Use reflection and rewards to drive a tree search over agents\\n  * Self-Discover Agent: Analyze an agent that learns about its own capabilities\\n\\n#### Evaluation¶\\n\\n  * Agent-based: Evaluate chatbots via simulated user interactions\\n  * In LangSmith: Evaluate chatbots in LangSmith over a dialog dataset\\n\\n#### Experimental¶\\n\\n  * Web Research (STORM): Generate Wikipedia-like articles via research and multi-perspective QA\\n  * TNT-LLM: Build rich, interpretable taxonomies of user intentand using the classification system developed by Microsoft for their Bing Copilot application.\\n  * Web Navigation: Build an agent that can navigate and interact with websites\\n  * Competitive Programming: Build an agent with few-shot \"episodic memory\" and human-in-the-loop collaboration to solve problems from the USA Computing Olympiad; adapted from the \"Can Language Models Solve Olympiad Programming?\" paper by Shi, Tang, Narasimhan, and Yao.\\n  * Complex data extraction: Build an agent that can use function calling to do complex extraction tasks\\n\\n## Comments\\n\\nBack to top\\n\\nPrevious\\n\\n🦜🕸️LangGraph\\n\\nNext\\n\\nQuick Start\\n\\nMade with  Material for MkDocs\\n\\n'), Document(metadata={'source': 'https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141', 'title': \"Mastering LangGraph: A Beginner's Guide to Building Intelligent Language Models | Medium\", 'description': \"Unlock the power of LangGraph with our beginner's guide. Learn to build stateful applications with LLMs and enhance your AI projects with expert tips. Master LangGraph now!\", 'language': 'en'}, page_content='Open in app\\n\\nSign up\\n\\nSign in\\n\\nWrite\\n\\nSign up\\n\\nSign in\\n\\n# Introduction to LangGraph: A Beginner’s Guide\\n\\nCPlog\\n\\n·\\n\\nFollow\\n\\n3 min read\\n\\n·\\n\\nFeb 15, 2024\\n\\n\\\\--\\n\\n3\\n\\nListen\\n\\nShare\\n\\nLangGraph is a powerful tool for building stateful, multi-actor applications\\nwith Large Language Models (LLMs). It extends the LangChain library, allowing\\nyou to coordinate multiple chains (or actors) across multiple steps of\\ncomputation in a cyclic manner. In this article, we’ll introduce LangGraph,\\nwalk you through its basic concepts, and share some insights and common points\\nof confusion for beginners.\\n\\n# What is LangGraph?\\n\\nLangGraph is a library built on top of LangChain, designed to add cyclic\\ncomputational capabilities to your LLM applications. While LangChain allows\\nyou to define chains of computation (Directed Acyclic Graphs or DAGs),\\nLangGraph introduces the ability to add cycles, enabling more complex, agent-\\nlike behaviors where you can call an LLM in a loop, asking it what action to\\ntake next.\\n\\n# Key Concepts\\n\\n  * **Stateful Graph:** LangGraph revolves around the concept of a stateful graph, where each node in the graph represents a step in your computation, and the graph maintains a state that is passed around and updated as the computation progresses.\\n  * **Nodes:** Nodes are the building blocks of your LangGraph. Each node represents a function or a computation step. You define nodes to perform specific tasks, such as processing input, making decisions, or interacting with external APIs.\\n  * **Edges:** Edges connect the nodes in your graph, defining the flow of computation. LangGraph supports conditional edges, allowing you to dynamically determine the next node to execute based on the current state of the graph.\\n\\n# A Simple Example\\n\\nLet’s walk through a simple example where we use LangGraph to classify user\\ninput as either a “greeting” or a “search” query and respond accordingly.\\n\\n# Step 1: Define the Graph State\\n\\nFirst, we define the state structure for our graph. In this example, our state\\nincludes the user’s question, the classification of the question, and a\\nresponse.\\n\\n    \\n    \\n    from typing import Dict, TypedDict, Optional  \\n    class GraphState(TypedDict):  \\n        question: Optional[str] = None  \\n        classification: Optional[str] = None  \\n        response: Optional[str] = None\\n\\n# Step 2: Create the Graph\\n\\nNext, we create a new instance of `StateGraph` with our `GraphState`\\nstructure.\\n\\n    \\n    \\n    from langgraph.graph import StateGraph  \\n    workflow = StateGraph(GraphState)\\n\\n# Step 3: Define Nodes\\n\\nWe define nodes for classifying the input, handling greetings, and handling\\nsearch queries.\\n\\n    \\n    \\n    def classify_input_node(state):  \\n        question = state.get(\\'question\\', \\'\\').strip()  \\n        classification = classify(question)  # Assume a function that classifies the input  \\n        return {\"classification\": classification}  \\n    def handle_greeting_node(state):  \\n        return {\"response\": \"Hello! How can I help you today?\"}  \\n    def handle_search_node(state):  \\n        question = state.get(\\'question\\', \\'\\').strip()  \\n        search_result = f\"Search result for \\'{question}\\'\"  \\n        return {\"response\": search_result}\\n\\n# Step 4: Add Nodes to the Graph\\n\\nWe add our nodes to the graph and define the flow using edges and conditional\\nedges.\\n\\n    \\n    \\n    workflow.add_node(\"classify_input\", classify_input_node)  \\n    workflow.add_node(\"handle_greeting\", handle_greeting_node)  \\n    workflow.add_node(\"handle_search\", handle_search_node)  \\n      \\n    def decide_next_node(state):  \\n        return \"handle_greeting\" if state.get(\\'classification\\') == \"greeting\" else \"handle_search\"  \\n    workflow.add_conditional_edges(  \\n        \"classify_input\",  \\n        decide_next_node,  \\n        {  \\n            \"handle_greeting\": \"handle_greeting\",  \\n            \"handle_search\": \"handle_search\"  \\n        }  \\n    )\\n\\n# Step 5: Set Entry and End Points\\n\\nWe set the entry point for our graph and define the end points.\\n\\n    \\n    \\n    workflow.set_entry_point(\"classify_input\")  \\n    workflow.add_edge(\\'handle_greeting\\', END)  \\n    workflow.add_edge(\\'handle_search\\', END)\\n\\n# Step 6: Compile and Run the Graph\\n\\nFinally, we compile our graph and run it with some input.\\n\\n    \\n    \\n    app = workflow.compile()  \\n    inputs = {\"question\": \"Hello, how are you?\"}  \\n    result = app.invoke(inputs)  \\n    print(result)\\n\\n# Common Confusions\\n\\n  * **State Management:** Understanding how the state is passed around and updated in the graph can be tricky. Remember that each node receives the current state, can modify it, and passes it on to the next node.\\n  * **Conditional Edges:** Setting up conditional edges requires careful consideration of the conditions and the mapping of outcomes to the next nodes. Ensure that the keys returned by the condition function match the keys in the conditional edge mapping.\\n  * **Dead-End Nodes:** Every node in the graph should have a path leading to another node or to the `END` node. If a node has no outgoing edges, it\\'s considered a dead-end, and you\\'ll need to add an edge to avoid errors.\\n\\n# Conclusion\\n\\nLangGraph is a versatile tool for building complex, stateful applications with\\nLLMs. By understanding its core concepts and working through simple examples,\\nbeginners can start to leverage its power for their projects. Remember to pay\\nattention to state management, conditional edges, and ensuring there are no\\ndead-end nodes in your graph. Happy coding!\\n\\nLangchain\\n\\nLlm\\n\\nAI\\n\\nChatbots\\n\\nPython\\n\\n\\\\--\\n\\n\\\\--\\n\\n3\\n\\nFollow\\n\\n## Written by CPlog\\n\\n48 Followers\\n\\nAI enthusiast & Principal Data Scientist at LFX & Li & Fung. I innovate with\\nLLMs, LangChain, Stable Diffusion & ComfyUI to revolutionize supply chains.\\n\\nFollow\\n\\nHelp\\n\\nStatus\\n\\nAbout\\n\\nCareers\\n\\nPress\\n\\nBlog\\n\\nPrivacy\\n\\nTerms\\n\\nText to speech\\n\\nTeams\\n\\n')], 'page_summaries': ['### Quick Start Guide for LangGraph\\n\\n**Source:** [Introduction to LangGraph: A Beginner’s Guide](https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141)\\n\\n#### What is LangGraph?\\nLangGraph is a powerful tool for building stateful, multi-actor applications with Large Language Models (LLMs). It extends the LangChain library, allowing you to coordinate multiple chains (or actors) across multiple steps of computation in a cyclic manner.\\n\\n#### Key Concepts\\n- **Stateful Graph:** Each node in the graph represents a step in your computation, and the graph maintains a state that is passed around and updated as the computation progresses.\\n- **Nodes:** Nodes are the building blocks of your LangGraph. Each node represents a function or a computation step.\\n- **Edges:** Edges connect the nodes in your graph, defining the flow of computation. LangGraph supports conditional edges, allowing you to dynamically determine the next node to execute based on the current state of the graph.\\n\\n#### Steps to Create a Simple LangGraph Application\\n\\n1. **Define the Graph State**\\n   ```python\\n   from typing import Dict, TypedDict, Optional\\n   class GraphState(TypedDict):\\n       question: Optional[str] = None\\n       classification: Optional[str] = None\\n       response: Optional[str] = None\\n   ```\\n\\n2. **Create the Graph**\\n   ```python\\n   from langgraph.graph import StateGraph\\n   workflow = StateGraph(GraphState)\\n   ```\\n\\n3. **Define Nodes**\\n   ```python\\n   def classify_input_node(state):\\n       question = state.get(\\'question\\', \\'\\').strip()\\n       classification = classify(question)  # Assume a function that classifies the input\\n       return {\"classification\": classification}\\n\\n   def handle_greeting_node(state):\\n       return {\"response\": \"Hello! How can I help you today?\"}\\n\\n   def handle_search_node(state):\\n       question = state.get(\\'question\\', \\'\\').strip()\\n       search_result = f\"Search result for \\'{question}\\'\"\\n       return {\"response\": search_result}\\n   ```\\n\\n4. **Add Nodes to the Graph**\\n   ```python\\n   workflow.add_node(\"classify_input\", classify_input_node)\\n   workflow.add_node(\"handle_greeting\", handle_greeting_node)\\n   workflow.add_node(\"handle_search\", handle_search_node)\\n\\n   def decide_next_node(state):\\n       return \"handle_greeting\" if state.get(\\'classification\\') == \"greeting\" else \"handle_search\"\\n\\n   workflow.add_conditional_edges(\\n       \"classify_input\",\\n       decide_next_node,\\n       {\\n           \"handle_greeting\": \"handle_greeting\",\\n           \"handle_search\": \"handle_search\"\\n       }\\n   )\\n   ```\\n\\n5. **Set Entry and End Points**\\n   ```python\\n   workflow.set_entry_point(\"classify_input\")\\n   workflow.add_edge(\\'handle_greeting\\', END)\\n   workflow.add_edge(\\'handle_search\\', END)\\n   ```\\n\\n6. **Compile and Run the Graph**\\n   ```python\\n   app = workflow.compile()\\n   inputs = {\"question\": \"Hello, how are you?\"}\\n   result = app.invoke(inputs)\\n   print(result)\\n   ```\\n\\n#### Common Confusions\\n- **State Management:** Understanding how the state is passed around and updated in the graph can be tricky.\\n- **Conditional Edges:** Setting up conditional edges requires careful consideration of the conditions and the mapping of outcomes to the next nodes.\\n- **Dead-End Nodes:** Every node in the graph should have a path leading to another node or to the `END` node.\\n\\n#### Conclusion\\nLangGraph is a versatile tool for building complex, stateful applications with LLMs. By understanding its core concepts and working through simple examples, beginners can start to leverage its power for their projects.', \"To create a quick start guide for LangGraph, you can refer to the tutorial available at [Datacamp's LangGraph Tutorial](https://www.datacamp.com/tutorial/langgraph-tutorial). This guide will help you get started with LangGraph by enabling JavaScript and cookies.\", '### Quick Start Guide for LangGraph\\n\\n**Source:** [LangGraph Tutorials](https://langchain-ai.github.io/langgraph/tutorials/)\\n\\n#### Quick Start\\n- **Overview:** Learn the basics of LangGraph by building an agent from scratch.\\n- **Link:** [Quick Start](https://langchain-ai.github.io/langgraph/tutorials/)\\n\\n#### Use Cases\\n- **Chatbots:**\\n  - **Customer Support:** Build a chatbot for managing flights, hotel reservations, car rentals, etc.\\n  - **Prompt Generation:** Create an information gathering chatbot.\\n  - **Code Assistant:** Develop a code analysis and generation assistant.\\n\\n- **Multi-Agent Systems:**\\n  - **Collaboration:** Enable two agents to work together on a task.\\n  - **Supervision:** Use an LLM to orchestrate and delegate tasks to individual agents.\\n  - **Hierarchical Teams:** Organize nested teams of agents to solve problems.\\n\\n- **RAG (Retrieval-Augmented Generation):**\\n  - **Adaptive RAG:** Implement adaptive RAG using local LLMs.\\n  - **Agentic RAG, Corrective RAG, Self-RAG:** Various RAG implementations using local LLMs.\\n  - **SQL Agent:** Build an SQL-based agent.\\n\\n- **Planning Agents:**\\n  - **Plan-and-Execute:** Implement a basic planning and execution agent.\\n  - **Reasoning without Observation:** Reduce re-planning by saving observations as variables.\\n  - **LLMCompiler:** Stream and eagerly execute a DAG of tasks from a planner.\\n\\n- **Reflection & Critique:**\\n  - **Basic Reflection:** Prompt the agent to reflect on and revise its outputs.\\n  - **Reflexion:** Critique missing and superfluous details to guide next steps.\\n  - **Language Agent Tree Search:** Use reflection and rewards to drive a tree search over agents.\\n  - **Self-Discover Agent:** Analyze an agent that learns about its own capabilities.\\n\\n- **Evaluation:**\\n  - **Agent-based:** Evaluate chatbots via simulated user interactions.\\n  - **In LangSmith:** Evaluate chatbots in LangSmith over a dialog dataset.\\n\\n- **Experimental:**\\n  - **Web Research (STORM):** Generate Wikipedia-like articles via research and multi-perspective QA.\\n  - **TNT-LLM:** Build rich, interpretable taxonomies of user intent.\\n  - **Web Navigation:** Build an agent that can navigate and interact with websites.\\n  - **Competitive Programming:** Build an agent with few-shot \"episodic memory\" and human-in-the-loop collaboration.\\n  - **Complex data extraction:** Build an agent that can use function calling to do complex extraction tasks.\\n\\nThis guide provides a comprehensive overview of LangGraph\\'s capabilities and how to get started with various use cases.', '### Quick Start Guide for LangGraph\\n\\n**Source:** [LangGraph Quick Start Guide](https://langchain-ai.github.io/langgraph/cloud/quick_start/)\\n\\n#### Set up Requirements\\n- **LLM:** Anthropic (sign up and get an API key)\\n- **Search Engine:** Tavily (sign up and get an API key)\\n- **Hosting:** LangSmith (sign up and get an API key)\\n\\n#### Set up Local Files\\n1. **Create a new application directory:**\\n   - `agent.py` or `agent.ts`: Code for your LangGraph agent.\\n   - `requirements.txt` or `package.json`: Dependencies for your graph.\\n   - `langgraph.json`: Configuration file for LangGraph.\\n   - `.env`: Environment variables with API keys.\\n\\n2. **Example `agent.py`/`agent.ts`:**\\n   ```python\\n   from langchain_anthropic import ChatAnthropic\\n   from langchain_community.tools.tavily_search import TavilySearchResults\\n   from langgraph.prebuilt import create_react_agent\\n\\n   model = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\\n   tools = [TavilySearchResults(max_results=2)]\\n   graph = create_react_agent(model, tools)\\n   ```\\n\\n3. **Example `requirements.txt`/`package.json`:**\\n   ```python\\n   langgraph\\n   langchain_anthropic\\n   tavily-python\\n   langchain_community\\n   ```\\n\\n4. **Example `langgraph.json`:**\\n   ```json\\n   {\\n     \"dependencies\": [\".\"],\\n     \"graphs\": {\\n       \"agent\": \"./agent.py:graph\"\\n     },\\n     \"env\": \".env\"\\n   }\\n   ```\\n\\n5. **Example `.env`:**\\n   ```\\n   ANTHROPIC_API_KEY=...\\n   TAVILY_API_KEY=...\\n   ```\\n\\n#### Test the Graph Build Locally\\n- **Using LangGraph Studio Desktop (recommended):** Provides visualization, interaction, and debugging.\\n- **Using the LangGraph CLI:**\\n  1. Install the LangGraph CLI: `pip install langgraph-cli`\\n  2. Start the API server locally: `langgraph up`\\n  3. Test the server:\\n     ```bash\\n     curl --request POST \\\\\\n         --url http://localhost:8123/runs/stream \\\\\\n         --header \\'Content-Type: application/json\\' \\\\\\n         --data \\'{\\n         \"assistant_id\": \"agent\",\\n         \"input\": {\\n             \"messages\": [\\n                 {\\n                     \"role\": \"user\",\\n                     \"content\": \"How are you?\"\\n                 }\\n             ]\\n         },\\n         \"metadata\": {},\\n         \"config\": {\\n             \"configurable\": {}\\n         },\\n         \"multitask_strategy\": \"reject\",\\n         \"stream_mode\": [\\n             \"values\"\\n         ]\\n     }\\'\\n     ```\\n\\n#### Deploy to Cloud\\n1. **Push your code to GitHub:** Create a GitHub repository with your application files.\\n2. **Deploy from GitHub with LangGraph Cloud:**\\n   - Go to LangSmith and create a new deployment.\\n   - Connect LangGraph Cloud to GitHub.\\n   - Select your GitHub repository and configure the deployment settings.\\n\\n#### Inspect Traces + Monitor Service\\n- **Deployments View:** Access monitoring charts and recent traces.\\n- **Access the Docs:** Test API endpoints using your LangSmith API key.\\n\\n#### Interact with Your Deployment via LangGraph Studio\\n- **LangGraph Studio:** Test your graph by passing in starting states and exploring execution threads.\\n\\n#### Use with the SDK\\n1. **Install the SDK:** `pip install langgraph_sdk`\\n2. **Set up the client and execute a run:**\\n   ```python\\n   from langgraph_sdk import get_client\\n\\n   client = get_client(url=<DEPLOYMENT_URL>)\\n   assistants = await client.assistants.search()\\n   assistant = [a for a in assistants if not a[\"config\"]][0]\\n   thread = await client.threads.create()\\n\\n   input = {\"messages\":[{\"role\": \"user\", \"content\": \"Hello! My name is Bagatur and I am 26 years old.\"}]}\\n\\n   async for chunk in client.runs.stream(\\n           thread[\\'thread_id\\'],\\n           assistant[\"assistant_id\"],\\n           input=input,\\n           stream_mode=\"updates\",\\n       ):\\n       if chunk.data and chunk.event != \"metadata\":\\n           print(chunk.data)\\n   ```\\n\\n#### What\\'s Next\\n- **LangGraph Cloud How-tos:** Learn more about streaming, double-texting, and human-in-the-loop behavior.\\n- **LangGraph Tutorials:** Explore tutorials on writing LangGraph graphs and various use cases.', '### Quick Start Guide for LangGraph\\n\\n**Source:** [LangGraph Tutorials](https://langchain-ai.github.io/langgraph/tutorials/)\\n\\n#### Quick Start\\n- **Overview:** Learn the basics of LangGraph by building an agent from scratch.\\n- **Link:** [Quick Start](https://langchain-ai.github.io/langgraph/tutorials/)\\n\\n#### Use Cases\\n- **Chatbots:**\\n  - **Customer Support:** Build a chatbot for managing flights, hotel reservations, car rentals, etc.\\n  - **Prompt Generation:** Create an information gathering chatbot.\\n  - **Code Assistant:** Develop a code analysis and generation assistant.\\n\\n- **Multi-Agent Systems:**\\n  - **Collaboration:** Enable two agents to work together on a task.\\n  - **Supervision:** Use an LLM to orchestrate and delegate tasks to individual agents.\\n  - **Hierarchical Teams:** Organize nested teams of agents to solve problems.\\n\\n- **RAG (Retrieval-Augmented Generation):**\\n  - **Adaptive RAG:** Implement adaptive RAG using local LLMs.\\n  - **Agentic RAG, Corrective RAG, Self-RAG:** Various RAG implementations using local LLMs.\\n  - **SQL Agent:** Build an SQL-based agent.\\n\\n- **Planning Agents:**\\n  - **Plan-and-Execute:** Implement a basic planning and execution agent.\\n  - **Reasoning without Observation:** Reduce re-planning by saving observations as variables.\\n  - **LLMCompiler:** Stream and eagerly execute a DAG of tasks from a planner.\\n\\n- **Reflection & Critique:**\\n  - **Basic Reflection:** Prompt the agent to reflect on and revise its outputs.\\n  - **Reflexion:** Critique missing and superfluous details to guide next steps.\\n  - **Language Agent Tree Search:** Use reflection and rewards to drive a tree search over agents.\\n  - **Self-Discover Agent:** Analyze an agent that learns about its own capabilities.\\n\\n- **Evaluation:**\\n  - **Agent-based:** Evaluate chatbots via simulated user interactions.\\n  - **In LangSmith:** Evaluate chatbots in LangSmith over a dialog dataset.\\n\\n- **Experimental:**\\n  - **Web Research (STORM):** Generate Wikipedia-like articles via research and multi-perspective QA.\\n  - **TNT-LLM:** Build rich, interpretable taxonomies of user intent.\\n  - **Web Navigation:** Build an agent that can navigate and interact with websites.\\n  - **Competitive Programming:** Build an agent with few-shot \"episodic memory\" and human-in-the-loop collaboration.\\n  - **Complex data extraction:** Build an agent that can use function calling to do complex extraction tasks.\\n\\nThis guide provides a comprehensive overview of LangGraph\\'s capabilities and how to get started with various use cases.', '### Quick Start Guide for LangGraph\\n\\n**Source:** [Introduction to LangGraph: A Beginner’s Guide](https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141)\\n\\n#### What is LangGraph?\\nLangGraph is a powerful tool for building stateful, multi-actor applications with Large Language Models (LLMs). It extends the LangChain library, allowing you to coordinate multiple chains (or actors) across multiple steps of computation in a cyclic manner.\\n\\n#### Key Concepts\\n- **Stateful Graph:** Each node in the graph represents a step in your computation, and the graph maintains a state that is passed around and updated as the computation progresses.\\n- **Nodes:** Nodes are the building blocks of your LangGraph. Each node represents a function or a computation step.\\n- **Edges:** Edges connect the nodes in your graph, defining the flow of computation. LangGraph supports conditional edges, allowing you to dynamically determine the next node to execute based on the current state of the graph.\\n\\n#### Steps to Create a Simple LangGraph Application\\n\\n1. **Define the Graph State**\\n   ```python\\n   from typing import Dict, TypedDict, Optional\\n   class GraphState(TypedDict):\\n       question: Optional[str] = None\\n       classification: Optional[str] = None\\n       response: Optional[str] = None\\n   ```\\n\\n2. **Create the Graph**\\n   ```python\\n   from langgraph.graph import StateGraph\\n   workflow = StateGraph(GraphState)\\n   ```\\n\\n3. **Define Nodes**\\n   ```python\\n   def classify_input_node(state):\\n       question = state.get(\\'question\\', \\'\\').strip()\\n       classification = classify(question)  # Assume a function that classifies the input\\n       return {\"classification\": classification}\\n\\n   def handle_greeting_node(state):\\n       return {\"response\": \"Hello! How can I help you today?\"}\\n\\n   def handle_search_node(state):\\n       question = state.get(\\'question\\', \\'\\').strip()\\n       search_result = f\"Search result for \\'{question}\\'\"\\n       return {\"response\": search_result}\\n   ```\\n\\n4. **Add Nodes to the Graph**\\n   ```python\\n   workflow.add_node(\"classify_input\", classify_input_node)\\n   workflow.add_node(\"handle_greeting\", handle_greeting_node)\\n   workflow.add_node(\"handle_search\", handle_search_node)\\n\\n   def decide_next_node(state):\\n       return \"handle_greeting\" if state.get(\\'classification\\') == \"greeting\" else \"handle_search\"\\n\\n   workflow.add_conditional_edges(\\n       \"classify_input\",\\n       decide_next_node,\\n       {\\n           \"handle_greeting\": \"handle_greeting\",\\n           \"handle_search\": \"handle_search\"\\n       }\\n   )\\n   ```\\n\\n5. **Set Entry and End Points**\\n   ```python\\n   workflow.set_entry_point(\"classify_input\")\\n   workflow.add_edge(\\'handle_greeting\\', END)\\n   workflow.add_edge(\\'handle_search\\', END)\\n   ```\\n\\n6. **Compile and Run the Graph**\\n   ```python\\n   app = workflow.compile()\\n   inputs = {\"question\": \"Hello, how are you?\"}\\n   result = app.invoke(inputs)\\n   print(result)\\n   ```\\n\\n#### Common Confusions\\n- **State Management:** Understanding how the state is passed around and updated in the graph can be tricky.\\n- **Conditional Edges:** Setting up conditional edges requires careful consideration of the conditions and the mapping of outcomes to the next nodes.\\n- **Dead-End Nodes:** Every node in the graph should have a path leading to another node or to the `END` node.\\n\\n#### Conclusion\\nLangGraph is a versatile tool for building complex, stateful applications with LLMs. By understanding its core concepts and working through simple examples, beginners can start to leverage its power for their projects.']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display\n",
        "display(Markdown(output['messages'][-1].content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zvJtgGcLIyCI",
        "outputId": "7ef2d221-4d11-4dc2-fdde-f9daf4554b15"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here's a quick start guide for LangGraph:\n\n1. Install LangGraph:\n   ```\n   pip install langgraph\n   ```\n   [Source: Inferred from context]\n\n2. Define the Graph State:\n   ```python\n   from typing import Dict, TypedDict, Optional\n   class GraphState(TypedDict):\n       question: Optional[str] = None\n       classification: Optional[str] = None\n       response: Optional[str] = None\n   ```\n   [Source: https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141]\n\n3. Create the Graph:\n   ```python\n   from langgraph.graph import StateGraph\n   workflow = StateGraph(GraphState)\n   ```\n   [Source: https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141]\n\n4. Define Nodes:\n   ```python\n   def classify_input_node(state):\n       question = state.get('question', '').strip()\n       classification = classify(question)  # Assume a function that classifies the input\n       return {\"classification\": classification}\n\n   def handle_greeting_node(state):\n       return {\"response\": \"Hello! How can I help you today?\"}\n\n   def handle_search_node(state):\n       question = state.get('question', '').strip()\n       search_result = f\"Search result for '{question}'\"\n       return {\"response\": search_result}\n   ```\n   [Source: https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141]\n\n5. Add Nodes to the Graph:\n   ```python\n   workflow.add_node(\"classify_input\", classify_input_node)\n   workflow.add_node(\"handle_greeting\", handle_greeting_node)\n   workflow.add_node(\"handle_search\", handle_search_node)\n\n   def decide_next_node(state):\n       return \"handle_greeting\" if state.get('classification') == \"greeting\" else \"handle_search\"\n\n   workflow.add_conditional_edges(\n       \"classify_input\",\n       decide_next_node,\n       {\n           \"handle_greeting\": \"handle_greeting\",\n           \"handle_search\": \"handle_search\"\n       }\n   )\n   ```\n   [Source: https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141]\n\n6. Set Entry and End Points:\n   ```python\n   workflow.set_entry_point(\"classify_input\")\n   workflow.add_edge('handle_greeting', END)\n   workflow.add_edge('handle_search', END)\n   ```\n   [Source: https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141]\n\n7. Compile and Run the Graph:\n   ```python\n   app = workflow.compile()\n   inputs = {\"question\": \"Hello, how are you?\"}\n   result = app.invoke(inputs)\n   print(result)\n   ```\n   [Source: https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141]\n\n8. For more advanced use cases and tutorials, visit the official LangGraph Tutorials page: [https://langchain-ai.github.io/langgraph/tutorials/]\n\nKey Concepts to Remember:\n- Stateful Graph: The graph maintains a state that is passed around and updated as the computation progresses.\n- Nodes: Represent functions or computation steps in your graph.\n- Edges: Connect nodes and define the flow of computation, including conditional edges.\n[Source: https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141]\n\nThis quick start guide should help you get started with LangGraph and create a simple application. For more complex use cases and detailed tutorials, refer to the official documentation and tutorials."
          },
          "metadata": {}
        }
      ]
    }
  ]
}