---
module-name: pipelines_server
description: A containerized service that manages and executes various data processing and AI pipelines
related-modules:
  - open-webui
  - ollama

technologies:
  - Docker
  - Python
  - FastAPI
  - LangChain
  - Pandas
  - Pydantic

conventions:
  pipeline_structure:
    - Each pipeline should be a separate Python file in the pipelines/ directory
    - Pipeline files are automatically discovered and mounted into the container
    - Pipeline URLs are generated based on filename patterns

  deployment:
    - Uses Docker for containerization
    - Exposes port 9099 for API access
    - Automatically handles container lifecycle management
    - Mounts pipeline files and requirements dynamically

architecture:
  components:
    - Docker container running the pipelines service
    - FastAPI backend for pipeline execution
    - Dynamic pipeline loading system
    - Host-container file mounting for pipeline definitions

development:
  setup:
    - Ensure Docker is installed and running
    - Place pipeline files in pipelines/ directory
    - Update requirements.txt as needed for new dependencies
    - Use deploy_pipelines_server.sh for deployment

  pipeline_development:
    - Create new .py files in pipelines/ directory
    - Follow LangChain patterns for pipeline implementation
    - Ensure all dependencies are listed in requirements.txt

deployment:
  requirements:
    - Docker
    - Host port 9099 must be available
    - Proper file permissions for mounted directories

  configuration:
    - Container auto-restarts on failure
    - Automatic pipeline URL generation
    - Dynamic requirements installation
    - Environment variable configuration support

maintenance:
  logs:
    - Access via 'docker logs pipelines'
    - Container health status reported during deployment
    - Automatic cleanup of old containers on redeployment

dependencies:
  python_packages:
    - langchain ecosystem for AI/ML pipelines
    - pandas for data processing
    - fastapi for API endpoints
    - pydantic for data validation
    - various utility packages for specific pipeline needs
---

The Pipelines Server module serves as a containerized service for managing and executing various data processing and AI pipelines. It provides a flexible architecture where pipelines can be dynamically loaded and executed through a standardized API interface.

## Key Features

- Dynamic pipeline discovery and loading
- Containerized execution environment
- Automatic requirements management
- Support for various pipeline types (LangChain, Pandas, etc.)

## Pipeline Development

Pipelines are Python files placed in the `pipelines/` directory. Each pipeline is automatically discovered and made available through the service. The system supports various types of pipelines, with a focus on LangChain and data processing workflows.

## Deployment

The module includes a deployment script (`deploy_pipelines_server.sh`) that handles:
- Container lifecycle management
- Pipeline mounting and discovery
- Requirements installation
- Health checking

## Integration Points

The service integrates with other stack components through:
- HTTP API endpoints
- File system mounting for pipeline definitions
- Environment variable configuration
